#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Agent for Binary Code Perturbation
åŸºäº Proximal Policy Optimization çš„äºŒè¿›åˆ¶ä»£ç å˜å¼‚æ™ºèƒ½ä½“

ä¾èµ–: pip install torch numpy pandas
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.distributions import Categorical
import json
import os
from collections import deque
from loguru import logger

class StructuredJointNetwork(nn.Module):
    def __init__(self, state_dim, n_actions, n_locs, hidden_dim=512):
        super(StructuredJointNetwork, self).__init__()
        
        # === 1. ç‰¹å¾åˆ‡ç‰‡å®šä¹‰ (æ ¹æ® env_wrapper.py) ===
        # Part 1 (16): History
        # Part 2 (40): Topology
        # Part 3 (128): Critical Semantics (Top-3 * 32 + Context)
        # Part 4 (72): Global Semantics
        
        self.block_feat_start = 56
        self.block_feat_end = 152
        self.block_dim = 32
        self.num_blocks = 3

        # === ç¼–ç å™¨ ===
        # å—ç‰¹å¾ç¼–ç å™¨ï¼ˆå…±äº«æƒé‡ï¼‰
        self.block_encoder = nn.Sequential(
            nn.Linear(self.block_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 64),
            nn.ReLU()
        )
        
        # å…¨å±€ç‰¹å¾ç¼–ç å™¨
        self.global_input_dim = state_dim - (self.block_dim * self.num_blocks)
        self.global_encoder = nn.Sequential(
            nn.Linear(self.global_input_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 256),
            nn.ReLU()
        )
        
        # === è”åˆå†³ç­–å¤´ ===
        self.fusion_dim = 256 + (64 * self.num_blocks)

        # âœ… ä¸åŠ  Softmaxï¼Œè¾“å‡º logits
        self.actor_head = nn.Sequential(
            nn.Linear(self.fusion_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, n_locs * n_actions)
        )
        
        # Criticï¼šä½¿ç”¨ä¸ actor ç›¸åŒçš„èåˆè¡¨å¾ï¼Œå¢åŠ ä¸€å±‚è¡¨è¾¾èƒ½åŠ›
        self.critic = nn.Sequential(
            nn.Linear(self.fusion_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, state, sampled_loc_idx=None):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            state: [B, state_dim] çŠ¶æ€ç‰¹å¾
            sampled_loc_idx: [B] æˆ– None
                - è®­ç»ƒæ—¶ï¼šä¼ å…¥é‡‡æ ·çš„ location ç´¢å¼•ï¼ˆç¡¬é€‰æ‹©ï¼‰
                - æ¨ç†æ—¶ï¼šä½¿ç”¨ Noneï¼ˆè½¯æ³¨æ„åŠ›ï¼‰
        
        è¿”å›:
            action_probs: [B, n_actions] åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼ˆlogitsï¼‰
            loc_probs: [B, 3] ä½ç½®æ¦‚ç‡åˆ†å¸ƒ
            state_value: [B, 1] çŠ¶æ€ä»·å€¼
        """
        batch_size = state.size(0)

        # 1. æå–ç‰¹å¾
        blocks_raw = state[:, self.block_feat_start:self.block_feat_end]
        blocks_view = blocks_raw.view(batch_size, self.num_blocks, self.block_dim)

        global_raw = torch.cat([
            state[:, :self.block_feat_start], 
            state[:, self.block_feat_end:]
        ], dim=1)
        
        # 2. ç¼–ç 
        global_emb = self.global_encoder(global_raw)
        
        block_embs = []
        for i in range(self.num_blocks):
            b_emb = self.block_encoder(blocks_view[:, i, :])
            block_embs.append(b_emb)
        
        blocks_concat = torch.cat(block_embs, dim=1)
        
        # 3. èåˆä¸è¾“å‡º
        fusion = torch.cat([global_emb, blocks_concat], dim=1)
        action_logits = self.actor_head(fusion)  # âœ… logits
        state_value = self.critic(fusion)
        
        return action_logits, state_value


class PPOAgent:
    """PPO æ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim=256, n_actions=None, n_locs=3, lr=1e-4, gamma=0.99, 
                 epsilon=0.2, epochs=10, device='cpu', action_map=None):
        """
        å‚æ•°:
            state_dim: çŠ¶æ€ç»´åº¦ï¼ˆç‰¹å¾å‘é‡é•¿åº¦ï¼‰
            n_actions: åŠ¨ä½œæ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ action_map çš„é•¿åº¦ï¼‰
            n_locs: ä½ç½®æ•°é‡ï¼ˆé»˜è®¤3ï¼šTop-3ï¼‰
            lr: å­¦ä¹ ç‡ï¼ˆé™ä½åˆ° 1e-4ï¼‰
            gamma: æŠ˜æ‰£å› å­
            epsilon: PPOè£å‰ªå‚æ•°
            epochs: æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
            device: 'cpu' æˆ– 'cuda'
            action_map: åŠ¨ä½œæ˜ å°„åˆ—è¡¨ï¼ˆç´¢å¼• -> å®é™…å˜å¼‚æ¨¡å¼ï¼‰
        """
        self.device = torch.device(device)
        self.gamma = gamma
        self.epsilon = epsilon
        self.epochs = epochs
        
        # åŠ¨ä½œæ˜ å°„ï¼šç´¢å¼• -> å®é™…å˜å¼‚æ¨¡å¼ï¼ˆä¸ env_wrapper ä¿æŒä¸€è‡´ï¼‰
        default_action_map = [1, 2, 4, 7, 8, 9, 11, 13, 14, 15, 16]
        if action_map is None:
            action_map = list(default_action_map)

        if n_actions is not None:
            if n_actions <= 0:
                raise ValueError("n_actions must be positive")
            if n_actions > len(action_map):
                logger.warning(
                    f"[ppo_agent.py:init]: n_actions ({n_actions}) > action_map size "
                    f"({len(action_map)}), clamping to {len(action_map)}"
                )
                n_actions = len(action_map)
            action_map = action_map[:n_actions]

        self.action_map = action_map
        self.n_actions = len(self.action_map) 
        self.n_locs = n_locs

        # åˆå§‹åŒ–ç½‘ç»œ
        self.policy = StructuredJointNetwork(state_dim, self.n_actions, self.n_locs).to(self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        self.action_stats = np.zeros((self.n_locs, self.n_actions))
        
        # ç»éªŒç¼“å†²
        self.memory = {
            'states': [],
            'joint_actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': [],
            'loc_masks': []
        }
    
    def select_action(self, state, explore=True, loc_mask=None):
        """
       
        """
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action_logits, state_value = self.policy(state)
        # ä½ç½®æ©ç ï¼ˆä»…åœ¨é‡‡æ ·æ—¶ä½¿ç”¨ï¼‰
        action_logits = self._apply_loc_mask(action_logits, loc_mask)
        
        dist = Categorical(logits=action_logits)
        
        if explore:
            joint_action = dist.sample()
        else:
            joint_action = torch.argmax(action_logits, dim=-1)
        
        log_prob = dist.log_prob(joint_action)
        joint_idx = joint_action.item()
        
        # è§£ç 
        loc_idx = joint_idx // self.n_actions
        act_idx = joint_idx % self.n_actions
        actual_action = self.action_map[act_idx]
        
        # ç»Ÿè®¡
        if explore:
            self.action_stats[loc_idx, act_idx] += 1
        
        return joint_idx, loc_idx, act_idx, actual_action, log_prob.item(), state_value.item()

    def estimate_value(self, state):
        """
        ä¼°è®¡å•ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼ˆç”¨äºæˆªæ–­å›åˆçš„ bootstrapï¼‰
        
        å‚æ•°:
            state: å½“å‰çŠ¶æ€ç‰¹å¾å‘é‡
        
        è¿”å›:
            value: çŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            _, state_value = self.policy(state)  # åªéœ€è¦ valueï¼Œä¸éœ€è¦ logits
        return state_value.item()
    
    def store_transition(self, state, joint_action, reward, log_prob, value, done, loc_mask=None):
        self.memory['states'].append(state)
        self.memory['joint_actions'].append(joint_action)
        self.memory['rewards'].append(reward)
        self.memory['log_probs'].append(log_prob)
        self.memory['values'].append(value)
        self.memory['dones'].append(1.0 if done else 0.0)
        if loc_mask is None:
            loc_mask = [1] * self.n_locs
        self.memory['loc_masks'].append(loc_mask)
    
    def compute_returns(self, next_value=0):
        """è®¡ç®—å›æŠ¥ï¼ˆä½¿ç”¨ GAE - Generalized Advantage Estimationï¼‰"""
        returns = []
        advantages = []
        gae = 0
        
        rewards = self.memory['rewards']
        values = self.memory['values'] + [next_value]
        dones = self.memory['dones']

        # åå‘è®¡ç®— GAE
        for i in reversed(range(len(rewards))):
            mask = 1.0 - float(dones[i])
            delta = rewards[i] + self.gamma * mask * values[i + 1] - values[i]
            gae = delta + self.gamma * 0.95 * mask * gae  # lambda=0.95
            
            # æ£€æŸ¥è®¡ç®—ç»“æœ
            if np.isnan(gae) or np.isinf(gae):
                logger.error(f"[ppo_agent.py:compute_returns]: NaN/Inf in gae at step {i}")
                logger.error(f"  delta={delta}, gae={gae}, rewards[{i}]={rewards[i]}, values[{i}]={values[i]}, values[{i+1}]={values[i+1]}")
                # ä½¿ç”¨ 0 ä½œä¸ºåå¤‡
                gae = 0.0
            
            returns.insert(0, gae + values[i])
            advantages.insert(0, gae)
        
        # å…ˆè®°å½•æœªå½’ä¸€åŒ–çš„ä¼˜åŠ¿ç»Ÿè®¡ï¼Œç”¨äºè¯Šæ–­æ˜¯å¦åœ¨â€œçœŸå­¦ä¹ â€
        raw_adv = np.array(advantages, dtype=np.float32) if advantages else np.zeros(1, dtype=np.float32)
        adv_stats = {
            'adv_mean_raw': float(raw_adv.mean()),
            'adv_std_raw': float(raw_adv.std()),
            'adv_abs_mean_raw': float(np.abs(raw_adv).mean()),
            'adv_max_abs_raw': float(np.abs(raw_adv).max())
        }

        # åœ¨è¿™é‡Œå½’ä¸€åŒ– Advantageï¼ˆç”¨äºè®­ç»ƒç¨³å®šæ€§ï¼‰
        advantages = torch.FloatTensor(advantages).to(self.device)
        # âœ… å½’ä¸€åŒ–å‰æ£€æŸ¥æ–¹å·®
        if len(advantages) > 1:
            adv_std = advantages.std()
            if adv_std > 1e-8:
                advantages = (advantages - advantages.mean()) / (adv_std + 1e-8)
            else:
                logger.warning("Advantagesæ–¹å·®ä¸º0ï¼Œè·³è¿‡å½’ä¸€åŒ–")
        return returns, advantages, adv_stats
    
    def update(self, next_value=0.0):
        """PPO æ›´æ–°ç­–ç•¥"""
        if len(self.memory['states']) == 0:
            return {
                'loss': 0.0,
                'adv_mean_raw': 0.0,
                'adv_std_raw': 0.0,
                'adv_abs_mean_raw': 0.0,
                'adv_max_abs_raw': 0.0
            }
        
        # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        returns, advantages, adv_stats = self.compute_returns(next_value=next_value)
        
        # è½¬æ¢ä¸º tensor
        states = torch.FloatTensor(np.array(self.memory['states'])).to(self.device)
        joint_actions = torch.LongTensor(self.memory['joint_actions']).to(self.device)
        old_log_probs = torch.FloatTensor(self.memory['log_probs']).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)
        
        total_loss_val = 0
        
        for epoch in range(self.epochs):
            action_logits, state_values = self.policy(states)
            
            # âœ… ç®€å•è£å‰ªï¼Œä¸å½’ä¸€åŒ–
            action_logits = torch.clamp(action_logits, -20, 20)
            # âœ… ä½ç½®æ©ç ï¼šå±è”½æ— æ•ˆä½ç½®
            if self.memory.get('loc_masks'):
                loc_masks = self.memory['loc_masks']
                loc_masks = torch.FloatTensor(np.array(loc_masks)).to(self.device)
                action_logits = self._apply_loc_mask(action_logits, loc_masks)
            
            # âœ… NaN æ£€æŸ¥
            if torch.isnan(action_logits).any():
                logger.error(f"Epoch {epoch}: NaN in logits!")
                self.clear_memory()
                return {
                    'loss': 0.0,
                    **adv_stats
                }
            
            # âœ… ä½¿ç”¨ logits
            dist = Categorical(logits=action_logits)
            new_log_probs = dist.log_prob(joint_actions)
            entropy = dist.entropy().mean()

           # Actor Loss (PPO Clip)
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages
            
            # Stronger entropy bonus helps prevent early collapse to one action.
            actor_loss = -torch.min(surr1, surr2).mean() - 0.05 * entropy
            
            # Critic Loss
            state_values_sq = state_values.squeeze()
            if state_values_sq.dim() == 0:
                state_values_sq = state_values_sq.unsqueeze(0)

            min_len = min(state_values_sq.shape[0], returns.shape[0])
            critic_loss = nn.functional.smooth_l1_loss(
                state_values_sq[:min_len], 
                returns[:min_len]
            )

            # æ€» Loss
            loss = actor_loss + 0.5 * critic_loss
            
            # âœ… æ¢¯åº¦æ£€æŸ¥
            if torch.isnan(loss):
                logger.error(f"Epoch {epoch}: NaN in loss!")
                self.clear_memory()
                return {
                    'loss': 0.0,
                    **adv_stats
                }
            

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()
            
            total_loss_val += loss.item()

        self.clear_memory()
        return {
            'loss': total_loss_val / self.epochs,
            **adv_stats
        }

    def log_action_distribution(self, episode):
        """
        è¯Šæ–­å·¥å…·ï¼šåˆ†æåŠ¨ä½œåˆ†å¸ƒ
        ç”¨äºæ£€æµ‹ç­–ç•¥æ˜¯å¦é€€åŒ–æˆå‡åŒ€åˆ†å¸ƒ
        è¾“å‡ºå†™å…¥ log/action_distribution.log æ–‡ä»¶
        """
        if episode % 50 != 0 or episode == 0:
            return
        
        total = self.action_stats.sum()
        if total < 10:
            return
        
        # ç¡®å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„
        log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'log')
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, 'action_distribution.log')
        
        # å†™å…¥æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"ğŸ“Š åŠ¨ä½œåˆ†å¸ƒåˆ†æ (Episode {episode})\n")
            f.write("=" * 60 + "\n")
            
            # è®¡ç®—ç†µ
            probs = self.action_stats.flatten() / total
            probs = probs[probs > 0]
            entropy = -np.sum(probs * np.log(probs))
            max_entropy = np.log(self.n_locs * self.n_actions)
            
            f.write(f"ç­–ç•¥ç†µ: {entropy:.3f} / {max_entropy:.3f} ({entropy/max_entropy:.1%})\n")
            
            # Top-5 ç»„åˆ
            flat_indices = np.argsort(self.action_stats.flatten())[::-1][:5]
            f.write("\nğŸ† Top-5 æœ€å¸¸ç”¨ç»„åˆ:\n")
            for rank, flat_idx in enumerate(flat_indices, 1):
                loc_idx = flat_idx // self.n_actions
                act_idx = flat_idx % self.n_actions
                count = self.action_stats.flatten()[flat_idx]
                ratio = count / total
                f.write(
                    f"  #{rank}: ä½ç½®{loc_idx} Ã— åŠ¨ä½œ{act_idx} "
                    f"(å®é™…åŠ¨ä½œ={self.action_map[act_idx]}) | {ratio:.2%}\n"
                )
            
            # ä½ç½®åå¥½
            loc_dist = self.action_stats.sum(axis=1) / total
            f.write(f"\nğŸ“ ä½ç½®é€‰æ‹©åˆ†å¸ƒ: {loc_dist}\n")
            
            # åŠ¨ä½œåå¥½
            act_dist = self.action_stats.sum(axis=0) / total
            f.write(f"âš¡ åŠ¨ä½œé€‰æ‹©åˆ†å¸ƒ: {act_dist}\n")
            
            # è­¦å‘Š
            if entropy > max_entropy * 0.95:
                f.write("âš ï¸ ç†µè¿‡é«˜ï¼ç­–ç•¥æ¥è¿‘éšæœºé€‰æ‹©ï¼ˆå¯èƒ½æœªæ”¶æ•›ï¼‰\n")
            elif entropy < max_entropy * 0.2:
                f.write("âš ï¸ ç†µè¿‡ä½ï¼ç­–ç•¥å¯èƒ½è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£\n")
            else:
                f.write("âœ… ç­–ç•¥ç†µæ­£å¸¸ï¼Œæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡è‰¯å¥½\n")
            
            f.write("=" * 60 + "\n\n")
        
        # é‡ç½®
        self.action_stats.fill(0)

    def clear_memory(self):
        """æ¸…ç©ºç»éªŒç¼“å†²"""
        self.memory = {
            'states': [],
            'joint_actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': [],
            'loc_masks': []
        }

    def _apply_loc_mask(self, action_logits, loc_mask):
        """
        å°†ä½ç½®æ©ç åº”ç”¨åˆ° joint action logits ä¸Šã€‚
        loc_mask: shape [B, n_locs] æˆ– [n_locs]
        """
        if loc_mask is None:
            return action_logits

        if not torch.is_tensor(loc_mask):
            loc_mask = torch.tensor(loc_mask, dtype=action_logits.dtype, device=action_logits.device)
        if loc_mask.dim() == 1:
            loc_mask = loc_mask.unsqueeze(0)
        if loc_mask.size(-1) != self.n_locs:
            logger.warning(
                f"[ppo_agent.py:_apply_loc_mask] loc_mask size {loc_mask.size(-1)} "
                f"!= n_locs {self.n_locs}, skip mask"
            )
            return action_logits

        # é˜²æ­¢å…¨ 0 æ©ç å¯¼è‡´æ— æ³•é‡‡æ ·ï¼šå…¨ 0 è¡Œæ”¹ä¸ºå…¨ 1
        mask_sum = loc_mask.sum(dim=1, keepdim=True)
        loc_mask = torch.where(mask_sum > 0, loc_mask, torch.ones_like(loc_mask))
        joint_mask = loc_mask.repeat_interleave(self.n_actions, dim=1)
        return action_logits.masked_fill(joint_mask <= 0, -1e9)
    
    def save(self, path, extra_state=None):
        """ä¿å­˜æ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        payload = {
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'action_stats': self.action_stats,
            'hyperparams': {
                'gamma': self.gamma,
                'epsilon': self.epsilon,
                'epochs': self.epochs
            }
        }
        if extra_state:
            payload['trainer_state'] = extra_state
        torch.save(payload, path)
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        if not os.path.exists(path):
            logger.warning(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return
        
        checkpoint = torch.load(path, map_location=self.device)

        # åŠ è½½ç½‘ç»œæƒé‡ï¼ˆå…¼å®¹åŠ¨ä½œç©ºé—´å˜æ›´å¯¼è‡´çš„ head å°ºå¯¸ä¸ä¸€è‡´ï¼‰
        ckpt_state = checkpoint.get('policy_state_dict', {})
        model_state = self.policy.state_dict()
        compatible = {}
        skipped = []
        for k, v in ckpt_state.items():
            if k in model_state and model_state[k].shape == v.shape:
                compatible[k] = v
            else:
                skipped.append(k)

        model_state.update(compatible)
        self.policy.load_state_dict(model_state, strict=False)
        if skipped:
            logger.warning(
                f"âš ï¸ æ£€æµ‹åˆ°ç»“æ„ä¸å…¼å®¹å‚æ•°ï¼Œå·²è·³è¿‡åŠ è½½ {len(skipped)} é¡¹ï¼ˆå¸¸è§äºåŠ¨ä½œæ•°å˜åŒ–ï¼‰"
            )
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼Œç»“æ„å˜åŒ–æ—¶å¯èƒ½å¤±è´¥ï¼‰
        if 'optimizer_state_dict' in checkpoint:
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except Exception as e:
                logger.warning(f"âš ï¸ ä¼˜åŒ–å™¨çŠ¶æ€ä¸å…¼å®¹ï¼Œè·³è¿‡åŠ è½½: {e}")
        
        # åŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if 'action_stats' in checkpoint:
            try:
                stats = checkpoint['action_stats']
                if isinstance(stats, np.ndarray) and stats.shape == self.action_stats.shape:
                    self.action_stats = stats
            except Exception:
                pass
        

if __name__ == "__main__":
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'log')
    print(log_dir)
