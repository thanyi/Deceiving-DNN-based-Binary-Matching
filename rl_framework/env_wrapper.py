#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Environment Wrapper for Binary Code Perturbation
äºŒè¿›åˆ¶ä»£ç å˜å¼‚ç¯å¢ƒåŒ…è£…å™¨ï¼ˆPython 3ï¼‰

åŠŸèƒ½ï¼š
- è°ƒç”¨ uroboros (Python 2) è¿›è¡Œä»£ç å˜å¼‚
- ä½¿ç”¨ run_utils (Python 3) è¿›è¡Œç›¸ä¼¼åº¦è¯„ä¼°
- æä¾›æ ‡å‡† RL ç¯å¢ƒæ¥å£
"""

import sys
import os
import json
import time
import subprocess
import numpy as np
import pickle
import hashlib
from loguru import logger
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ç°æœ‰æ¨¡å—
from run_utils import run_one
import run_objdump
from rl_framework.utils.acfg.r2_acfg_features import RadareACFGExtractor


class BinaryPerturbationEnv:
    """
    äºŒè¿›åˆ¶ä»£ç å˜å¼‚ç¯å¢ƒ (Python 3)
    
    ä¸ PPO Agent åœ¨åŒä¸€è¿›ç¨‹ä¸­è¿è¡Œï¼Œé€šè¿‡å‡½æ•°è°ƒç”¨é€šä¿¡
    """
    
    def __init__(self, save_path, dataset_path, sample_hold_interval=3):
        """
        å‚æ•°:
            original_binary: åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
            function_name: ç›®æ ‡å‡½æ•°å
            save_path: ä¿å­˜å˜å¼‚ç»“æœçš„è·¯å¾„
        """
        self.save_path = os.path.abspath(save_path)
        # é¡¹ç›®æ ¹ç›®å½•ï¼ˆuroboros æ‰€åœ¨ç›®å½•ï¼‰
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    

        # åŠ è½½æ•°æ®é›†
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"Dataset not found: {dataset_path}")
            
        with open(dataset_path, 'r') as f:
            self.dataset = json.load(f)
        
        logger.info(f"å·²åŠ è½½æ•°æ®é›†: {len(self.dataset)} ä¸ªæ ·æœ¬")
        
        # åˆ‡æ¢ç­–ç•¥æ§åˆ¶
        self.sample_hold_interval = sample_hold_interval
        self.episodes_on_current = 0
        self.current_sample_data = None # å­˜å‚¨å½“å‰æ ·æœ¬çš„å…ƒæ•°æ®
        
        # å½“å‰ç¯å¢ƒçŠ¶æ€å˜é‡
        self.original_binary = None # åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
        self.function_name = None # ç›®æ ‡å‡½æ•°å
        self.current_binary = None # å½“å‰å˜å¼‚åçš„äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
        
        # ä¸å†éœ€è¦åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ asm2vec æ–¹æ³•ï¼‰
        self.model_original = None
        logger.info("Using asm2vec detection method (no model loading required)")
        
        # å˜å¼‚å†å²
        self.mutation_history = []
        self.step_count = 0
        self.target_score = 0.40
        self.state_dim = 256  # é»˜è®¤çŠ¶æ€ç»´åº¦ï¼ˆ256ç»´ï¼‰ï¼Œå¯ä»¥é€šè¿‡å‚æ•°ä¿®æ”¹
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘Radare2 ç‰¹å¾æå–ç¼“å­˜
        # ç¼“å­˜é”®: (binary_path, function_name, function_addr)
        # ç¼“å­˜å€¼: acfg_data (dict)
        self._acfg_cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘åŸå§‹æ–‡ä»¶æ±‡ç¼–ç¼“å­˜ï¼ˆåŸå§‹æ–‡ä»¶ä¸å˜ï¼Œå¯å¤ç”¨ï¼‰
        # ç¼“å­˜é”®: (original_binary, function_name, ori_sym_addr)
        # ç¼“å­˜å€¼: æ±‡ç¼–æ–‡ä»¶è·¯å¾„
        self._original_asm_cache = {}
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘å¤ç”¨ä¸´æ—¶ç›®å½•ï¼Œé¿å…é¢‘ç¹åˆ›å»ºåˆ é™¤
        # åœ¨ save_path ä¸‹åˆ›å»ºå›ºå®šå·¥ä½œç›®å½•
        self._asm_work_dir = os.path.join(self.save_path, '_asm_work')
        os.makedirs(self._asm_work_dir, exist_ok=True)
        
        logger.info(f"Environment initialized (Hold Strategy: {self.sample_hold_interval} eps)")
    
    def set_state_dim(self, state_dim):
        """
        è®¾ç½®çŠ¶æ€ç»´åº¦ï¼ˆç”¨äºä¸ PPO Agent ä¿æŒä¸€è‡´ï¼‰
        
        å‚æ•°:
            state_dim: çŠ¶æ€ç»´åº¦
        """
        self.state_dim = state_dim
        logger.info(f"çŠ¶æ€ç»´åº¦è®¾ç½®ä¸º: {state_dim}")


    def _resolve_mutated_address(self, binary_path):
        """
        æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè§£æå˜å¼‚åå‡½æ•°çš„çœŸå®åœ°å€
        è§£å†³ Strip æ–‡ä»¶æ— æ³•é€šè¿‡å‡½æ•°åå®šä½çš„é—®é¢˜
        """
        # 1. å¦‚æœæ˜¯åŸå§‹æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åŸå§‹åœ°å€
        # è¿™é‡Œå‡è®¾åŸå§‹æ–‡ä»¶æœª Stripï¼Œæˆ–è€…ä½ èƒ½é€šè¿‡å‡½æ•°åæ‰¾åˆ°
        if binary_path == self.original_binary:
            return None, self.function_name

        # 2. å¯»æ‰¾ sym_to_addr.pickle æ˜ å°„æ–‡ä»¶
        mutated_dir = os.path.dirname(binary_path)
        pickle_path = os.path.join(mutated_dir, "sym_to_addr.pickle")
        
        # æœ‰æ—¶å€™ uroboros ä¼šç”Ÿæˆåœ¨ä¸Šä¸€çº§
        if not os.path.exists(pickle_path):
            pickle_path = os.path.join(os.path.dirname(mutated_dir), "sym_to_addr.pickle")

        if not os.path.exists(pickle_path):
            # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œåªèƒ½è¿”å› Noneï¼Œåç»­é€»è¾‘ä¼šå°è¯•ç›²çŒœå…¥å£ç‚¹
            # logger.warning(f"Map file missing for {binary_path}")
            return None, None

        try:
            with open(pickle_path, 'rb') as f:
                addr_map = pickle.load(f)
            
            # å°è¯•è·å–ç›®æ ‡å‡½æ•°çš„åœ°å€
            # Uroboros çš„ map key å¯èƒ½æ˜¯åŸå§‹å‡½æ•°å
            if self.function_name in addr_map:
                addr_str = addr_map[self.function_name]
                if isinstance(addr_str, str):
                    return int(addr_str, 16), None
                return int(addr_str), None
            
            # å¦‚æœæ‰¾ä¸åˆ°ç›´æ¥åŒ¹é…ï¼Œå°è¯•å¯»æ‰¾ 'func_xxxx' æ ¼å¼
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å› None è®© r2 å°è¯• entry0
            return None, None
            
        except Exception as e:
            logger.error(f"Error resolving address: {e}")
            return None, None


    def extract_features_from_function(self, binary_path, function_name=None):
        """
        ç‰¹å¾æå–å‡½æ•° (256ç»´)
        ç»„æˆ: [å†å²ç‰¹å¾(16)] + [ACFGç‰¹å¾(240)]
        ç”¨äºç‰¹å¾æå–æ—¶ï¼ŒæŒ‡å®šå‡½æ•°åï¼Œè€Œä¸æ˜¯é€šè¿‡è§£æåœ°å€
        """
        features = []
        # ==========================================
        # Part 1: å˜å¼‚å†å²ä¸ç¯å¢ƒçŠ¶æ€ (16ç»´)
        # ==========================================
        # 1. Score History (5 dims)
        if self.mutation_history:
            scores = [m.get('score', 1.0) for m in self.mutation_history[-5:]]
            features.extend(scores)
            features.extend([1.0] * (5 - len(scores)))
        else:
            features.extend([1.0] * 5)
            
        # 2. Action Histogram (6 dims)
        action_counts = [0] * 6
        total = max(len(self.mutation_history), 1)
        for m in self.mutation_history:
            idx = m.get('action')
            if 0 <= idx < 6: action_counts[idx] += 1
        features.extend([c/total for c in action_counts])
        
        # 3. Progress (2 dims)
        features.append(self.step_count / 50.0)
        features.append(1.0 if self.step_count > 25 else 0.0)
        
        # 4. Global State (3 dims)
        features.append(1.0 if len(self.mutation_history) > 0 else 0.0) # Is Modified
        features.append(self.episodes_on_current / 20.0) # Dataset progress
        features.append(0.0) # Padding

        # ==========================================
        # Part 2: åŸºäº Radare2 çš„ ACFG ç‰¹å¾ (æ ¸å¿ƒ)
        # ==========================================
        
        # åˆå§‹åŒ–é»˜è®¤å‘é‡ (å…¨0) ç”¨äºå¤±è´¥æƒ…å†µ
        acfg_vec = [0.0] * (self.state_dim - len(features))
        
        try:
            # 1. æ£€æŸ¥åœ°å€
            if function_name is None:
                raise Exception("function_name is None, please specify function_name")
            target_name = function_name
            target_addr = None
            # print(f"binary_path: {binary_path}")
            # print(f"[env_wrapper.py:extract_features_from_function] target_name: {target_name}, target_addr: {target_addr}")
            # 2. ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ£€æŸ¥ç¼“å­˜
            cache_key = (os.path.abspath(binary_path), target_name, target_addr)
            acfg_data = self._acfg_cache.get(cache_key)
            
            if acfg_data is None:
                # ç¼“å­˜æœªå‘½ä¸­ï¼šè°ƒç”¨ R2 æå–
                self._cache_misses += 1
                r2_ext = RadareACFGExtractor(binary_path)
                acfg_data = r2_ext.get_acfg_features(function_name=target_name, function_addr=target_addr)
                r2_ext.close()
                
                # å­˜å…¥ç¼“å­˜ï¼ˆåªç¼“å­˜æˆåŠŸæå–çš„æ•°æ®ï¼‰
                if acfg_data:
                    self._acfg_cache[cache_key] = acfg_data
            else:
                # ç¼“å­˜å‘½ä¸­
                self._cache_hits += 1
                if self._cache_hits % 100 == 0:
                    total = self._cache_hits + self._cache_misses
                    hit_rate = self._cache_hits / total if total > 0 else 0.0
                    logger.debug(f"ACFG ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ç‡={hit_rate:.2%} (å‘½ä¸­={self._cache_hits}, æœªå‘½ä¸­={self._cache_misses})")
            
            if acfg_data:
                # logger.debug(f"acfg_data: {acfg_data}")
                acfg_vec = self._vectorize_acfg(acfg_data)
                
        except (FileNotFoundError, KeyError, ValueError, AttributeError) as e:
            logger.warning(f"Feature extraction failed for {binary_path}: {e}")
            # ä¿æŒå…¨0
        
        features.extend(acfg_vec)
        
        # æœ€ç»ˆæˆªæ–­æˆ–è¡¥é½åˆ° 256 ç»´
        if len(features) > self.state_dim:
            features = features[:self.state_dim]
        elif len(features) < self.state_dim:
            features.extend([0.0] * (self.state_dim - len(features)))

        # === ã€æ ¸å¿ƒä¿®å¤ã€‘æ•°æ®æ¸…æ´— ===
        # 1. è½¬ä¸º numpy æ•°ç»„
        features = np.array(features, dtype=np.float32)
        
        # 2. æ›¿æ¢ NaN ä¸º 0ï¼Œæ›¿æ¢ Infinity ä¸ºæœ€å¤§/æœ€å°æœ‰é™å€¼
        # é˜²æ­¢ä»»ä½•è®¡ç®—é”™è¯¯äº§ç”Ÿçš„ NaN ä¼ å…¥ç¥ç»ç½‘ç»œ
        features = np.nan_to_num(features, nan=0.0, posinf=100.0, neginf=-100.0)
        
        # 3. è£å‰ªæ•°å€¼èŒƒå›´ (Clip)
        # é˜²æ­¢æŸäº›ç‰¹å¾æ•°å€¼è¿‡å¤§ï¼ˆæ¯”å¦‚ total_instr çªç„¶å¾ˆå¤§ï¼‰ï¼Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
        # å°†æ‰€æœ‰ç‰¹å¾é™åˆ¶åœ¨ [-10, 100] ä¹‹é—´é€šå¸¸è¶³å¤Ÿäº†
        features = np.clip(features, -10.0, 100.0)
            
        return features




    def extract_features(self, binary_path):
        """
        ç‰¹å¾æå–å‡½æ•° (256ç»´)
        ç»„æˆ: [å†å²ç‰¹å¾(16)] + [ACFGç‰¹å¾(240)]
        """
        features = []
        
        # ==========================================
        # Part 1: å˜å¼‚å†å²ä¸ç¯å¢ƒçŠ¶æ€ (16ç»´)
        # ==========================================
        # 1. Score History (5 dims)
        if self.mutation_history:
            scores = [m.get('score', 1.0) for m in self.mutation_history[-5:]]
            features.extend(scores)
            features.extend([1.0] * (5 - len(scores)))
        else:
            features.extend([1.0] * 5)
            
        # 2. Action Histogram (6 dims)
        action_counts = [0] * 6
        total = max(len(self.mutation_history), 1)
        for m in self.mutation_history:
            idx = m.get('action')
            if 0 <= idx < 6: action_counts[idx] += 1
        features.extend([c/total for c in action_counts])
        
        # 3. Progress (2 dims)
        features.append(self.step_count / 50.0)
        features.append(1.0 if self.step_count > 25 else 0.0)
        
        # 4. Global State (3 dims)
        features.append(1.0 if len(self.mutation_history) > 0 else 0.0) # Is Modified
        features.append(self.episodes_on_current / 20.0) # Dataset progress
        features.append(0.0) # Padding

        # ==========================================
        # Part 2: åŸºäº Radare2 çš„ ACFG ç‰¹å¾ (æ ¸å¿ƒ)
        # ==========================================
        
        # åˆå§‹åŒ–é»˜è®¤å‘é‡ (å…¨0) ç”¨äºå¤±è´¥æƒ…å†µ
        acfg_vec = [0.0] * (self.state_dim - len(features))
        
        try:
            # 1. è§£æåœ°å€
            target_addr, target_name = self._resolve_mutated_address(binary_path)
            # print(f"target_addr: {target_addr}, target_name: {target_name}")
            # 2. ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ£€æŸ¥ç¼“å­˜
            cache_key = (os.path.abspath(binary_path), target_name, target_addr)
            acfg_data = self._acfg_cache.get(cache_key)
            
            if acfg_data is None:
                # ç¼“å­˜æœªå‘½ä¸­ï¼šè°ƒç”¨ R2 æå–
                self._cache_misses += 1
                r2_ext = RadareACFGExtractor(binary_path)
                acfg_data = r2_ext.get_acfg_features(function_name=target_name, function_addr=target_addr)
                r2_ext.close()
                
                # å­˜å…¥ç¼“å­˜ï¼ˆåªç¼“å­˜æˆåŠŸæå–çš„æ•°æ®ï¼‰
                if acfg_data:
                    self._acfg_cache[cache_key] = acfg_data
            else:
                # ç¼“å­˜å‘½ä¸­
                self._cache_hits += 1
                if self._cache_hits % 100 == 0:
                    total = self._cache_hits + self._cache_misses
                    hit_rate = self._cache_hits / total if total > 0 else 0.0
                    logger.debug(f"ACFG ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ç‡={hit_rate:.2%} (å‘½ä¸­={self._cache_hits}, æœªå‘½ä¸­={self._cache_misses})")
            
            if acfg_data:
                acfg_vec = self._vectorize_acfg(acfg_data)
                
        except (FileNotFoundError, KeyError, ValueError, AttributeError) as e:
            logger.warning(f"Feature extraction failed for {binary_path}: {e}")
            # ä¿æŒå…¨0
        
        features.extend(acfg_vec)
        
        # æœ€ç»ˆæˆªæ–­æˆ–è¡¥é½åˆ° 256 ç»´
        if len(features) > self.state_dim:
            features = features[:self.state_dim]
        elif len(features) < self.state_dim:
            features.extend([0.0] * (self.state_dim - len(features)))

        # === ã€æ ¸å¿ƒä¿®å¤ã€‘æ•°æ®æ¸…æ´— ===
        # 1. è½¬ä¸º numpy æ•°ç»„
        features = np.array(features, dtype=np.float32)
        
        # 2. æ›¿æ¢ NaN ä¸º 0ï¼Œæ›¿æ¢ Infinity ä¸ºæœ€å¤§/æœ€å°æœ‰é™å€¼
        # é˜²æ­¢ä»»ä½•è®¡ç®—é”™è¯¯äº§ç”Ÿçš„ NaN ä¼ å…¥ç¥ç»ç½‘ç»œ
        features = np.nan_to_num(features, nan=0.0, posinf=100.0, neginf=-100.0)
        
        # 3. è£å‰ªæ•°å€¼èŒƒå›´ (Clip)
        # é˜²æ­¢æŸäº›ç‰¹å¾æ•°å€¼è¿‡å¤§ï¼ˆæ¯”å¦‚ total_instr çªç„¶å¾ˆå¤§ï¼‰ï¼Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
        # å°†æ‰€æœ‰ç‰¹å¾é™åˆ¶åœ¨ [-10, 100] ä¹‹é—´é€šå¸¸è¶³å¤Ÿäº†
        features = np.clip(features, -10.0, 100.0)
            
        return features
    

    def _vectorize_acfg(self, data):
        """
        ã€256ç»´ ç»ˆæä¿®å¤ç‰ˆã€‘
        Part 1 (16ç»´): RL History (å·²åœ¨å¤–éƒ¨å¡«å……)
        Part 2 (40ç»´): Section A - Macro Topology
        Part 3 (128ç»´): Section B - Critical Semantics (Micro)
        Part 4 (72ç»´): Section C - Global Semantics (Macro & Fingerprints)
        """
        vec = []
        
        n_nodes = max(data.get('num_nodes', 0), 1.0)
        n_edges = data.get('num_edges', 0)
        complexity = data.get('cyclomatic_complexity', 0)
        bbs = list(data.get('basic_blocks', {}).values())
        top_critical_addrs = data.get('top_critical_blocks', [])
        fingerprints = data.get('fingerprints', {})
        
        total_instr = sum(b['n_instructions'] for b in bbs)
        safe_total = max(total_instr, 1.0)
        
        def safe_div(a, b): return a / b if b > 0 else 0

        # ==========================================
        # Section A: Macro Topology (40 dims)
        # ==========================================
        # 1. Scale (8 dims)
        vec.append(np.log1p(n_nodes))
        vec.append(np.log1p(n_edges))
        vec.append(safe_div(n_edges, n_nodes)) 
        vec.append(np.log1p(complexity))
        vec.append(safe_div(complexity, n_nodes))
        
        leaf_cnt = sum(1 for b in bbs if b.get('n_branch', 0) == 0)
        vec.append(safe_div(leaf_cnt, n_nodes))
        # æ±‡èšèŠ‚ç‚¹(å…¥åº¦>1)ä¼°ç®—: ç®€åŒ–ä¸º edge-node+1
        vec.append(safe_div(max(0, n_edges - n_nodes + 1), n_nodes))
        vec.append(np.log1p(safe_total)) # æ€»æŒ‡ä»¤æ•°ç§»åˆ°è¿™é‡Œ

        # 2. Distributions (32 dims)
        # ã€ä¿®æ­£ã€‘ç»Ÿä¸€ä½¿ç”¨ log1p å¤„ç† Momentsï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
        def get_moments_log(values):
            if not values: return [0.0]*4
            arr = np.array(values)
            # å¯¹åŸå§‹å€¼å– log1p åå†ç®—çŸ©ï¼Œæ‹‰å¹³é‡çº²
            log_arr = np.log1p(arr)
            return [np.mean(log_arr), np.max(log_arr), np.std(log_arr), np.median(log_arr)]

        dist_bet = [b.get('centrality_betweenness', 0) for b in bbs]
        dist_deg = [b.get('centrality_degree', 0) for b in bbs]
        dist_dom = [b.get('dominator_score', 0) for b in bbs]
        dist_size = [b.get('n_instructions', 0) for b in bbs]

        for dist in [dist_bet, dist_deg, dist_dom, dist_size]:
            # 4 moments (log scaled)
            vec.extend(get_moments_log(dist))
            # Top-4 values (raw values, but ratio based for bet/deg)
            s_dist = sorted(dist, reverse=True)
            top4 = s_dist[:4] + [0.0]*(4-len(s_dist))
            # åªæœ‰ dominator å’Œ size æ˜¯ç»å¯¹å€¼ï¼Œéœ€è¦ logï¼›bet/deg æ˜¯ 0-1
            if dist is dist_dom or dist is dist_size:
                vec.extend([np.log1p(x) for x in top4])
            else:
                vec.extend(top4)
        
        # Section A Total: 8 + 4*8 = 40. Correct.

        # ==========================================
        # Section B: Critical Semantics (128 dims)
        # ==========================================
        crit_vectors = []
        for addr in top_critical_addrs:
            if addr not in data.get('basic_blocks', {}): continue
            bb = data['basic_blocks'][addr]
            n_inst = max(bb.get('n_instructions', 0), 1.0)
            
            # --- 32 Atomic Features per Block ---
            v = []
            # [0] Scale
            v.append(np.log1p(n_inst))
            
            # [1-10] Instruction Types
            keys = ['n_arith', 'n_logic', 'n_branch', 'n_transfer', 
                    'n_xor', 'n_shift', 'n_cmp', 
                    'n_mem_write', 'n_mem_read', 'n_consts']
            for k in keys: v.append(safe_div(bb.get(k, 0), n_inst))
            
            # [11-15] Data Flow
            v.append(safe_div(bb.get('n_regs_gp', 0), 16.0))
            v.append(safe_div(bb.get('n_regs_vec', 0), 16.0))
            v.append(safe_div(bb.get('n_mem_write',0)+bb.get('n_mem_read',0), n_inst))
            comp_ops = bb.get('n_arith', 0) + bb.get('n_logic', 0) + bb.get('n_xor', 0)
            v.append(safe_div(comp_ops, n_inst))
            v.append(1.0 if bb.get('n_consts', 0) > 0 else 0.0) # Has Constant
            
            # [16-19] Topology
            v.append(bb.get('centrality_betweenness', 0))
            v.append(bb.get('centrality_degree', 0))
            v.append(np.log1p(bb.get('dominator_score', 0))) # Log
            # Relative Centrality (Node / Max_in_Graph)
            max_bet = max(dist_bet) if dist_bet else 1.0
            v.append(safe_div(bb.get('centrality_betweenness', 0), max_bet))
            
            # [20-23] Structure Flags
            v.append(1.0 if bb.get('n_branch', 0) > 0 else 0.0)
            v.append(1.0 if bb.get('n_branch', 0) > 2 else 0.0) # Multi-way
            v.append(1.0 if bb.get('n_transfer', 0) == 0 else 0.0) # Pure Compute
            v.append(1.0 if n_inst < 5 else 0.0)
            
            # [24-31] Advanced Fillers (No more padding!)
            # Opcode Entropy Proxy
            uniq_types = sum(1 for k in keys if bb.get(k, 0) > 0)
            v.append(uniq_types / 10.0)
            # Stack Heavy
            v.append(1.0 if bb.get('n_transfer', 0) > n_inst*0.4 else 0.0)
            # Loop Header Heuristic
            is_loop = 1.0 if (bb.get('n_branch', 0) > 0 and bb.get('centrality_betweenness', 0) > 0.2) else 0.0
            v.append(is_loop)
            # Logic Heavy
            v.append(safe_div(bb.get('n_logic', 0) + bb.get('n_xor', 0), n_inst))
            # Shift Heavy (Crypto indicator)
            v.append(safe_div(bb.get('n_shift', 0), n_inst))
            # Cmp Density
            v.append(safe_div(bb.get('n_cmp', 0), n_inst))
            # Mem Write vs Read Ratio
            v.append(safe_div(bb.get('n_mem_write', 0), bb.get('n_mem_read', 0) + 1.0))
            # Reg Diversity Proxy
            v.append(safe_div(bb.get('n_regs_gp', 0) + bb.get('n_regs_vec', 0), 8.0))

            crit_vectors.append(v)

        # èšåˆ (4 * 32 = 128)
        if crit_vectors:
            mat = np.array(crit_vectors)
            vec.extend(np.mean(mat, axis=0))
            vec.extend(np.max(mat, axis=0))
            vec.extend(np.std(mat, axis=0))
            # Top-1 distinctiveness (Top1 - Mean)
            vec.extend(mat[0] - np.mean(mat, axis=0))
        else:
            vec.extend([0.0] * 128)

        # ==========================================
        # === Section C: Global Semantics (72 dims) ===
        # ==========================================
        
        # 1. Opcode Ratios (12 dims)
        global_keys = ['n_arith', 'n_logic', 'n_branch', 'n_transfer', 
                       'n_xor', 'n_shift', 'n_cmp', 'n_mem_write', 'n_mem_read', 
                       'n_consts', 'n_regs_gp', 'n_regs_vec']
        global_sums = {k: sum(b.get(k, 0) for b in bbs) for k in global_keys}
        for k in global_keys:
            vec.append(safe_div(global_sums[k], safe_total))
            
        # 2. API & Strings (22 dims) -> å¢åŠ  Internal
        vec.append(np.log1p(fingerprints.get('n_calls', 0)))
        vec.append(np.log1p(fingerprints.get('n_strings', 0)))
        vec.append(safe_div(fingerprints.get('n_calls', 0), safe_total))
        vec.append(safe_div(fingerprints.get('n_strings', 0), safe_total))
        
        # 9 Categories (Added 'internal')
        api_cats = ['io', 'mem', 'str', 'sys', 'net', 'crypto', 'error', 'other', 'internal']
        apis = fingerprints.get('api_types', set())
        for cat in api_cats:
            vec.append(1.0 if cat in apis else 0.0) # Exist
            vec.append(0.0) # Count placeholder
            
        # 3. Block Size Dist (5 dims)
        sizes = [b['n_instructions'] for b in bbs]
        if sizes:
            vec.append(sum(1 for s in sizes if s < 5) / len(sizes))
            vec.append(sum(1 for s in sizes if 5 <= s < 15) / len(sizes))
            vec.append(sum(1 for s in sizes if 15 <= s < 50) / len(sizes))
            vec.append(sum(1 for s in sizes if s >= 50) / len(sizes))
            vec.append(np.max(sizes) / safe_total)
        else:
            vec.extend([0.0] * 5)
            
        # 4. Operand Type Dist (5 dims)
        n_imm = fingerprints.get('n_ops_imm', 0)
        n_reg = fingerprints.get('n_ops_reg', 0)
        n_mem = fingerprints.get('n_ops_mem', 0)
        total_ops = n_imm + n_reg + n_mem
        
        vec.append(safe_div(n_imm, total_ops))
        vec.append(safe_div(n_reg, total_ops))
        vec.append(safe_div(n_mem, total_ops))
        vec.append(safe_div(n_mem, n_reg + 1.0))
        vec.append(safe_div(n_imm, n_reg + 1.0))
        
        # 5. Global Ratios (10 dims)
        vec.append(safe_div(global_sums['n_mem_write'], global_sums['n_mem_read'] + 1.0))
        vec.append(safe_div(global_sums['n_arith'], global_sums['n_logic'] + 1.0))
        vec.append(safe_div(global_sums['n_regs_vec'], global_sums['n_regs_gp'] + 1.0))
        vec.append(1.0 if global_sums['n_regs_vec'] > 0 else 0.0)
        vec.append(1.0 if global_sums['n_consts'] > 5 else 0.0)
        vec.append(safe_div(global_sums['n_branch'], safe_total))
        vec.append(safe_div(global_sums['n_arith'] + global_sums['n_logic'], safe_total))
        vec.append(safe_div(global_sums['n_transfer'], safe_total))
        vec.extend([0.0] * 2) 

        # 6. ã€ä¿®å¤ã€‘ç§»é™¤å‡‘æ•°ç‰¹å¾ï¼Œå¡«å……æœ‰æ„ä¹‰çš„ç»Ÿè®¡ (18 dims)
        # ç”¨æ›´å¤šçš„åˆ†å¸ƒä¿¡æ¯å¡«æ»¡
        # æ¯”å¦‚ï¼šå…³é”®å—å æ€»å—æ•°çš„æ¯”ä¾‹ (Criticality Density)
        crit_block_ratio = safe_div(len(top_critical_addrs), len(bbs))
        vec.append(crit_block_ratio)
        
        # æ¯”å¦‚ï¼šæœ€å¤§åŸºæœ¬å—çš„æŒ‡ä»¤å æ¯” (Dominant Block Influence)
        max_bb_ratio = safe_div(max(sizes) if sizes else 0, safe_total)
        vec.append(max_bb_ratio)
        
        # å‰©ä¸‹çš„è¡¥ 0ï¼Œæˆ–è€…ä½ å¯ä»¥åŠ æ›´å¤šä¸šåŠ¡ç›¸å…³çš„ç‰¹å¾
        needed = 256 - 16 - len(vec) # æ³¨æ„ï¼šå¤–éƒ¨è¿˜æœ‰16ç»´
        # è¿™é‡Œ vec åŒ…å« A, B, Cã€‚é•¿åº¦åº”ä¸º 40+128+72 = 240
        # å®é™… len(vec) åº”è¯¥æ˜¯ 240
        # å¦‚æœé•¿åº¦ä¸å¤Ÿï¼Œè¡¥0
        if needed > 0: vec.extend([0.0] * needed)
        elif needed < 0: vec = vec[:256-16] # æˆªæ–­
            
        return vec


    
    def apply_mutation(self, seed_binary, action):
        """
        åº”ç”¨å˜å¼‚æ“ä½œ
        
        å‚æ•°:
            seed_binary: ç§å­äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
            action: å˜å¼‚æ¨¡å¼ (1,2,3,5,7,8,9,11)
        
        è¿”å›:
            mutated_binary: å˜å¼‚åçš„äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info("Applying mutation {} to {}".format(action, seed_binary))
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = '/home/ycy/ours/Deceiving-DNN-based-Binary-Matching/rl_framework/rl_output'
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆä¸´æ—¶äºŒè¿›åˆ¶æ–‡ä»¶å
            tmp_bin = os.path.join(output_dir, 'mutant_' + str(int(time.time() * 1000)) + '.bin')
            
            # ç¡®å®šæ¨¡å¼
            fmode = 'mutated' if seed_binary != self.original_binary else 'original'
            
            # ä¸ºæ¯æ¬¡å˜å¼‚åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•
            tmp_id = str(int(time.time() * 1000))
            tmp_dir = os.path.join(self.save_path, 'tmp_' + tmp_id)
            os.makedirs(tmp_dir, exist_ok=True)
            
            # è°ƒç”¨ uroboros (Python 2) è¿›è¡Œå˜å¼‚
            # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä½¿ç”¨ python2ï¼Œå› ä¸º uroboros æ˜¯ Python 2 ä»£ç 
            cmd = [
                'python2',
                os.path.join(self.project_root, 'uroboros_automate-func-name.py'),
                seed_binary,
                '-i', '1',  # è¿­ä»£æ¬¡æ•°
                '-o', tmp_bin,
                '-d', str(action),
                '-m', fmode,
                '-f', tmp_dir,
                '--function', self.function_name
            ]
            
            logger.debug("Command: " + " ".join(cmd))
            
            # åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œå‘½ä»¤
            try:
                output = subprocess.check_output(
                    cmd, 
                    stderr=subprocess.STDOUT,
                    cwd=self.project_root,
                    universal_newlines=True  # è¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯å­—èŠ‚
                )
                logger.debug("Uroboros output: {}".format(output))
            except subprocess.CalledProcessError as e:
                # æ•è·è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_output = e.output if hasattr(e, 'output') else str(e)
                logger.error("Uroboros command failed with exit code {}: {}".format(e.returncode, error_output))
                logger.error("Command was: {}".format(" ".join(cmd)))
                raise Exception("Uroboros mutation failed: {}\nOutput: {}".format(e, error_output))
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(tmp_bin):
                raise FileNotFoundError("Mutation output not found: {}".format(tmp_bin))
            
            # è®¡ç®— hash å¹¶ç§»åŠ¨æ–‡ä»¶
            h = hashlib.md5(open(tmp_bin, 'rb').read()).hexdigest()
            container_path = os.path.join(self.save_path, h + '_container')
            
            if not os.path.exists(container_path):
                # ç§»åŠ¨ tmp ç›®å½•åˆ° container
                import shutil
                shutil.move(tmp_dir, container_path)
                # ç§»åŠ¨äºŒè¿›åˆ¶æ–‡ä»¶
                shutil.move(tmp_bin, os.path.join(container_path, h))
            
            mutated_binary = os.path.join(container_path, h)
            logger.info("Mutation successful: {}".format(mutated_binary))
            
            return mutated_binary, h
            
        except Exception as e:
            logger.error("Mutation failed: {}".format(e))
            return None, None
    
    def evaluate(self, mutated_binary, checkdict):
        """
        è¯„ä¼°å˜å¼‚åçš„äºŒè¿›åˆ¶æ–‡ä»¶
        
        è¿”å›:
            score: ç›¸ä¼¼åº¦åˆ†æ•°
            grad: æ¢¯åº¦å€¼
        """
        try:
            # ã€æ€§èƒ½ä¼˜åŒ–ã€‘ä¼ é€’å·¥ä½œç›®å½•å’Œç¼“å­˜ï¼Œé¿å…æ¯æ¬¡åˆ›å»ºä¸´æ—¶ç›®å½•
            score, grad = run_one(
                self.original_binary,
                mutated_binary,
                self.model_original,
                checkdict,
                self.function_name,
                asm_work_dir=self._asm_work_dir,
                original_asm_cache=self._original_asm_cache
            )
            
            if score is None or grad is None:
                logger.warning("Evaluation returned None")
                return 1.0, 0.0  # é»˜è®¤æœ€å·®å€¼
            
            return abs(score), abs(grad)
            
        except Exception as e:
            logger.error("Evaluation failed: {}".format(e))
            return 1.0, 0.0
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥ç¯å¢ƒäº¤äº’
        
        å‚æ•°:
            action: å˜å¼‚æ¨¡å¼
        
        è¿”å›:
            state: æ–°çŠ¶æ€ç‰¹å¾
            reward: å¥–åŠ±
            done: æ˜¯å¦å®Œæˆ
            info: é¢å¤–ä¿¡æ¯
        """
        self.step_count += 1
        # è®°å½•ä¸Šä¸€æ­¥åˆ†æ•°ï¼Œç”¨äºè®¡ç®—å·®åˆ†å¥–åŠ±
        prev_score = self.mutation_history[-1]['score'] if self.mutation_history else 1.0
        
        # åº”ç”¨å˜å¼‚
        mutated_binary, hash_val = self.apply_mutation(self.current_binary, action)
        
        if mutated_binary is None:
            # å˜å¼‚å¤±è´¥ï¼šæ ‡è®°éœ€è¦é‡ç½®ç¯å¢ƒå¹¶åˆ‡æ¢æ–‡ä»¶
            logger.warning("Mutation failed, will reset environment and switch to new file")
            state = self.extract_features(self.current_binary)
            return state, -10.0, True, {
                'error': 'mutation_failed',
                'should_reset': True,  # æ ‡å¿—ï¼šéœ€è¦é‡ç½®å¹¶åˆ‡æ¢æ–‡ä»¶
                'score': 1.0,  # é»˜è®¤æœ€å·®åˆ†æ•°
                'grad': 0.0
            }
        
        # è¯„ä¼°
        # TODO: è·å–æ­£ç¡®çš„ checkdict
        checkdict = {}  # éœ€è¦ä»å®é™…æ–‡ä»¶ä¸­åŠ è½½
        score, grad = self.evaluate(mutated_binary, checkdict)
        
        # æ›´æ–°çŠ¶æ€
        self.current_binary = mutated_binary
        self.mutation_history.append({
            'step': self.step_count,
            'action': action,
            'binary': mutated_binary,
            'hash': hash_val,
            'score': score,
            'grad': grad
        })
        
        # æå–æ–°çŠ¶æ€ç‰¹å¾
        state = self.extract_features(mutated_binary)
        
        # è®¡ç®—å¥–åŠ±
        reward = self.compute_reward_diff(prev_score, score, grad)
        # reward = self.compute_reward(score, grad)
        
        # åˆ¤æ–­æ˜¯å¦å®Œæˆ
        done = score < self.target_score or self.step_count >= 50
        
        info = {
            'score': score,
            'grad': grad,
            'step': self.step_count,
            'binary': mutated_binary,
            'target_func': self.function_name # è®°å½•å½“å‰ç›®æ ‡å‡½æ•°å
        }
        
        logger.info("Step {}: action={}, score={:.4f}, reward={:.4f}".format(
            self.step_count, action, score, reward
        ))
        
        return state, reward, done, info
    
    def compute_reward_diff(self, prev_score, current_score, grad):
        """
        å·®åˆ†å¥–åŠ±å‡½æ•°ï¼šé€‚åˆå¤šæ ·æœ¬è®­ç»ƒ
        """
        # 1. è¿›æ­¥å¥–åŠ± (å…³é”®)ï¼šåˆ†æ•°ä¸‹é™äº†å¤šå°‘
        improvement = prev_score - current_score
        
        # å¦‚æœè¿›æ­¥äº†ï¼Œç»™æ­£å¥–åŠ±ï¼›é€€æ­¥äº†ï¼Œç»™è´Ÿå¥–åŠ±
        # æ”¾å¤§ç³»æ•° 20ï¼Œè®© Agent å¯¹å¾®å°çš„è¿›æ­¥ä¹Ÿæ•æ„Ÿ
        reward = improvement * 20.0
        
        # 2. æˆåŠŸå¥–åŠ± (Jackpot)
        if current_score < self.target_score:
            reward += 50.0 
        
        # 3. æ­¥æ•°æƒ©ç½š (Time Penalty)
        reward -= 0.1
        
        # ã€ä¿®å¤ã€‘é™åˆ¶å¥–åŠ±èŒƒå›´ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        reward = np.clip(reward, -20.0, 50.0) 
        return reward




    def compute_reward(self, score, grad):
        """è®¡ç®—å¥–åŠ±"""
        # åŸºç¡€å¥–åŠ±
        reward = -score
        
        # æˆåŠŸå¥–åŠ±
        if score < self.target_score:
            reward += 10.0
        
        # æ¢¯åº¦å¥–åŠ±
        reward += abs(grad) * 0.1
        
        # æ­¥æ•°æƒ©ç½š
        reward -= self.step_count * 0.01
        
        return reward
    
    # def reset(self):
    #     """é‡ç½®ç¯å¢ƒ"""
    #     self.current_binary = self.original_binary
    #     self.mutation_history = []
    #     self.step_count = 0
        
    #     # æå–åˆå§‹ç‰¹å¾
    #     state = self.extract_features(self.original_binary)
    #     return state

    def reset(self, force_switch=False):
        """
        é‡ç½®ç¯å¢ƒï¼šå®ç°è‡ªåŠ¨åˆ‡æ¢ç›®æ ‡ (Hold-N Strategy)
        
        å‚æ•°:
            force_switch: å¦‚æœä¸º Trueï¼Œå¼ºåˆ¶åˆ‡æ¢ç›®æ ‡ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
        """
        # å¼ºåˆ¶åˆ‡æ¢ï¼ˆé”™è¯¯æ¢å¤ï¼‰ï¼šå¿½ç•¥ Hold-N ç­–ç•¥ï¼Œç›´æ¥åˆ‡æ¢ç›®æ ‡
        if force_switch:
            self.current_sample_data = random.choice(self.dataset)
            self.episodes_on_current = 0
            self.original_binary = self.current_sample_data['binary_path']
            self.function_name = self.current_sample_data['func_name']
            logger.warning(f"ğŸ”„ FORCE SWITCH (Error Recovery) -> {os.path.basename(self.original_binary)}::{self.function_name}")
            logger.info(f"   Version: {self.current_sample_data.get('version')} | Opt: {self.current_sample_data.get('opt_level')}")
        # æ­£å¸¸åˆ‡æ¢ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢ç›®æ ‡
        elif self.current_sample_data is None or self.episodes_on_current >= self.sample_hold_interval:
            # éšæœºæŠ½å–ä¸€ä¸ªæ–°æ ·æœ¬
            self.current_sample_data = random.choice(self.dataset)
            self.episodes_on_current = 0
            
            # æ›´æ–°ç¯å¢ƒé…ç½®
            self.original_binary = self.current_sample_data['binary_path']
            self.function_name = self.current_sample_data['func_name']
            
            logger.success(f"ğŸ”„ SWITCH TARGET -> {os.path.basename(self.original_binary)}::{self.function_name}")
            logger.info(f"   Version: {self.current_sample_data.get('version')} | Opt: {self.current_sample_data.get('opt_level')}")
        else:
            # ä¿æŒå½“å‰ç›®æ ‡ï¼Œå¢åŠ è®¡æ•°
            self.episodes_on_current += 1
            logger.info(f"ğŸ”„ KEEP TARGET ({self.episodes_on_current}/{self.sample_hold_interval}) -> {self.function_name}")

        # é‡ç½®ç¯å¢ƒçŠ¶æ€
        self.current_binary = self.original_binary
        self.mutation_history = []
        self.step_count = 0
        
        # æå–åˆå§‹ç‰¹å¾
        state = self.extract_features(self.original_binary)
        return state
    
    def clear_acfg_cache(self):
        """
        æ¸…ç† ACFG ç‰¹å¾ç¼“å­˜
        
        ç”¨äºé‡Šæ”¾å†…å­˜ï¼Œé€šå¸¸åœ¨åˆ‡æ¢å¤§é‡ä¸åŒç›®æ ‡æ—¶è°ƒç”¨
        """
        cache_size = len(self._acfg_cache)
        self._acfg_cache.clear()
        logger.info(f"å·²æ¸…ç† ACFG ç¼“å­˜: é‡Šæ”¾ {cache_size} ä¸ªæ¡ç›®")
    
    def get_cache_stats(self):
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›:
            dict: åŒ…å«å‘½ä¸­ç‡ã€å‘½ä¸­æ•°ã€æœªå‘½ä¸­æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
        """
        total = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total if total > 0 else 0.0
        return {
            'cache_size': len(self._acfg_cache),
            'cache_hits': self._cache_hits,
            'cache_misses': self._cache_misses,
            'hit_rate': hit_rate
        }

# if __name__ == "__main__":
#     # æµ‹è¯•ç”¨ä¾‹
#     import argparse
    
#     parser = argparse.ArgumentParser(description='Binary Perturbation Environment')
#     parser.add_argument('--binary', required=True, help='Original binary path')
#     parser.add_argument('--function', required=True, help='Target function name')
#     parser.add_argument('--save-path', required=True, help='Save path for mutations')
    
#     args = parser.parse_args()
    
#     env = BinaryPerturbationEnv(
#         original_binary=args.binary,
#         function_name=args.function,
#         save_path=args.save_path
#     )
    
#     logger.info("Environment initialized successfully")
    
#     # æµ‹è¯•é‡ç½®
#     state = env.reset()
#     logger.info("Initial state shape: {}".format(len(state)))
    
#     # æµ‹è¯•å•æ­¥
#     logger.info("Testing mutation with action=5...")
#     next_state, reward, done, info = env.step(5)
#     logger.info("Score: {:.4f}, Reward: {:.4f}".format(info.get('score', 0), reward))


if __name__ == '__main__':
    bin_path = '/home/ycy/ours/Deceiving-DNN-based-Binary-Matching/rl_framework/datasets/coreutils/bin/coreutils-8.15-O0/sort'
    DATASET_PATH = '/home/ycy/ours/Deceiving-DNN-based-Binary-Matching/rl_framework/utils/dataset_test.json'
    env = BinaryPerturbationEnv(save_path="/tmp/test_env", dataset_path=DATASET_PATH)
    state = env.extract_features_from_function(bin_path,'xstrtoumax')
    print(state)