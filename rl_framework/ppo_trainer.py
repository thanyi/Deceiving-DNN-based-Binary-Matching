#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Trainer for Binary Code Perturbation
PPO è®­ç»ƒå™¨ï¼ˆç›´æ¥è°ƒç”¨ç¯å¢ƒï¼‰
"""
import os
import numpy as np
import torch
from ppo_agent import PPOAgent
import argparse
from loguru import logger
import sys
import shutil
import glob
from collections import deque
from torch.utils.tensorboard import SummaryWriter

# å¯¼å…¥ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env_wrapper import BinaryPerturbationEnv


def cleanup_intermediate_files(save_path, episode_binaries=None):
    """
    æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ–‡ä»¶
    """
    if not os.path.exists(save_path):
        return
    
    logger.info(f"å¼€å§‹æ¸…ç†ä¸­é—´æ–‡ä»¶: {save_path}")
    
    # æå–éœ€è¦ä¿ç•™çš„äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
    binaries_to_keep = set()
    if episode_binaries:
        for item in episode_binaries:
            if 'binary' in item and item['binary']:
                binaries_to_keep.add(os.path.abspath(item['binary']))
        logger.info(f"å°†ä¿ç•™ {len(binaries_to_keep)} ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶")
    
    deleted_count = 0
    deleted_size = 0
    
    # æ¸…ç† tmp_* ä¸´æ—¶ç›®å½•
    tmp_pattern = os.path.join(save_path, 'tmp_*')
    for tmp_dir in glob.glob(tmp_pattern):
        if os.path.isdir(tmp_dir):
            try:
                size = sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, _, filenames in os.walk(tmp_dir) for filename in filenames)
                shutil.rmtree(tmp_dir)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½• {tmp_dir}: {e}")
    
    # æ¸…ç† *_container å®¹å™¨ç›®å½•
    container_pattern = os.path.join(save_path, '*_container')
    for container_dir in glob.glob(container_pattern):
        if os.path.isdir(container_dir):
            container_dir_abs = os.path.abspath(container_dir)
            should_keep = False
            
            if binaries_to_keep:
                for binary_path in binaries_to_keep:
                    if binary_path.startswith(container_dir_abs + os.sep) or binary_path == container_dir_abs:
                        should_keep = True
                        break
            
            if should_keep: continue
            
            try:
                size = sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, _, filenames in os.walk(container_dir) for filename in filenames)
                shutil.rmtree(container_dir)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤å®¹å™¨ç›®å½• {container_dir}: {e}")
    
    # æ¸…ç† rl_output ä¸­çš„ä¸­é—´æ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    rl_output_dir = os.path.join(current_dir, 'rl_output')
    if os.path.exists(rl_output_dir):
        mutant_files = glob.glob(os.path.join(rl_output_dir, 'mutant_*.bin*'))
        for file_path in mutant_files:
            try:
                size = os.path.getsize(file_path)
                os.remove(file_path)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                pass
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    if deleted_size < 1024: size_str = f"{deleted_size} B"
    elif deleted_size < 1024**2: size_str = f"{deleted_size/1024:.2f} KB"
    else: size_str = f"{deleted_size/1024**2:.2f} MB"
    
    logger.success(f"âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªé¡¹ç›®ï¼Œé‡Šæ”¾ {size_str} ç©ºé—´")


def train_ppo(args):
    """
    PPO è®­ç»ƒä¸»å‡½æ•°
    """
    logger.info("PPO è®­ç»ƒå¯åŠ¨ (Multi-Sample Mode)")
    logger.info(f"æ•°æ®é›†: {args.dataset}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    
    os.makedirs(args.save_path, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    tensorboard_dir = os.path.join(args.model_dir, 'tensorboard')
    writer = SummaryWriter(log_dir=tensorboard_dir)
    logger.info(f"TensorBoard: {tensorboard_dir}")
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    env = BinaryPerturbationEnv(
        save_path=args.save_path,
        dataset_path=args.dataset,
        sample_hold_interval=args.sample_hold_interval # Hold-N ç­–ç•¥
        
    )
    env.set_state_dim(args.state_dim)
    
    agent = PPOAgent(
        state_dim=args.state_dim,
        lr=args.lr,
        device='cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    )
    
    if args.resume and os.path.exists(args.resume):
        agent.load(args.resume)
    
    log_file = os.path.join(args.model_dir, 'training_log.txt')
    
    episode_binaries = []
    
    # æ»‘åŠ¨çª—å£ç»Ÿè®¡
    success_window = deque(maxlen=50)
    similarity_drop_window = deque(maxlen=50)
    
    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    success_count = 0
    best_score = float('inf')
    info = {}  # åˆå§‹åŒ– infoï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
   
    global_total_steps = 0 
    try:
        for episode in range(args.episodes):
            logger.info("=" * 60)
            logger.info(f"å›åˆ {episode + 1}/{args.episodes}")
            
            state = env.reset()
            
            episode_actions = [] 
            initial_score = 1.0 # ã€ä¼˜åŒ–ã€‘é»˜è®¤åˆå§‹ä¸º1.0ï¼Œé˜²æ­¢ç¬¬ä¸€æ­¥æ²¡å–åˆ°scoreå¯¼è‡´è®¡ç®—é”™è¯¯
            
            episode_reward = 0
            last_binary_info = None
            should_skip_update = False
            episode_done = False  # æ ‡è®° episode æ˜¯å¦æ­£å¸¸ç»“æŸ
            
            for step in range(args.max_steps):
                global_total_steps += 1 

                joint_idx, loc_idx, act_idx, actual_action, log_prob, value = agent.select_action(state, explore=True)
                episode_actions.append(actual_action)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(actual_action, loc_idx)
                
                episode_reward += reward
                state = next_state
                
                
                if step % 10 == 0:
                    # è®°å½•æ¯æ­¥æŒ‡æ ‡ (å½“å‰è®¾ç½®ï¼šæ¯æ­¥éƒ½è®°ï¼Œå¦‚æœå¤ªæ…¢å¯æ”¹ä¸º if step % 5 == 0)
                    writer.add_scalar('Step/Shaped_Reward', reward, global_total_steps)            # Agent æ¯åšä¸€æ­¥åŠ¨ä½œå¾—åˆ°çš„å³æ—¶åé¦ˆï¼ˆåŒ…å«è¿›æ­¥åˆ†ã€æƒ©ç½šåˆ†ç­‰ï¼‰ã€‚
                    writer.add_scalar('Step/Critic_Value', value, global_total_steps)                     # Critic ç½‘ç»œï¼ˆè£åˆ¤ï¼‰è®¤ä¸ºâ€œå½“å‰è¿™ä¸ªçŠ¶æ€ï¼Œæœªæ¥èƒ½æ‹¿å¤šå°‘åˆ†â€ã€‚
                    if 'score' in info:
                        writer.add_scalar('Step/Similarity_Score', info['score'], global_total_steps)     # æ¯ä¸€æ­¥å˜å¼‚åçš„ä»£ç ä¸åŸä»£ç çš„ç›¸ä¼¼åº¦ã€‚

                # å­˜å‚¨ç»éªŒ
                agent.store_transition(state, joint_idx, reward, log_prob, value, done)
 
                if 'binary' in info:
                    last_binary_info = {
                        'episode': episode, 'step': step,
                        'binary': info['binary'], 'score': info.get('score', 1.0),
                        'func': info.get('target_func', 'unknown') # å­˜ä¸€ä¸‹å‡½æ•°å
                    }
                
                # âœ… æˆåŠŸæ£€æŸ¥ä¸é”™è¯¯å¤„ç†
                if done:
                    if info.get('should_reset', False):
                        logger.warning("âš ï¸ é”™è¯¯å‘ç”Ÿï¼Œå¼ºåˆ¶åˆ‡æ¢ç›®æ ‡")
                        should_skip_update = True
                        state = env.reset(force_switch=True)
                    
                    episode_done = True
                    break
            
            # âœ… ç»Ÿä¸€çš„ç»Ÿè®¡é€»è¾‘
            # 1. ç»Ÿè®¡æˆåŠŸç‡å’Œé™åˆ† (ä½¿ç”¨ last_binary_info æ›´å®‰å…¨)
            final_score = last_binary_info['score'] if last_binary_info else 1.0
            target_func = last_binary_info['func'] if last_binary_info else "unknown"

            is_success = final_score < 0.40
            success_window.append(1 if is_success else 0)
            similarity_drop_window.append(max(0.0, initial_score - final_score))

            if is_success:
                success_count += 1
                logger.success(f"ğŸ‰ æ”»ç ´! ç›®æ ‡: {info.get('target_func')} | åˆ†æ•°: {final_score:.4f}")
                with open(os.path.join(args.save_path, 'success.log'), 'a') as f:
                    f.write(f"Ep {episode}, Func: {info.get('target_func')}, Score: {final_score:.4f}\n")

            if last_binary_info:
                episode_binaries.append(last_binary_info)

            # 2. å¦‚æœå‡ºé”™è·³è¿‡æ›´æ–°
            if should_skip_update:
                agent.clear_memory()
                continue
            
            # æˆªæ–­å›åˆæ—¶åš bootstrap
            next_value = 0.0
            if not episode_done:
                next_value = agent.estimate_value(state)
            
            # PPO æ›´æ–°
            loss = agent.update(next_value=next_value)

            # æ‰“å°åŠ¨ä½œåˆ†å¸ƒ
            agent.log_action_distribution(episode)
            
            # === Episode çº§åˆ«è®°å½• (æ ¸å¿ƒ) ===
            current_success_rate = np.mean(success_window) if success_window else 0.0
            avg_drop = np.mean(similarity_drop_window) if similarity_drop_window else 0.0
            
            logger.info(f"å›åˆæ€»ç»“: æ€»å¥–={episode_reward:.2f} | æ»‘åŠ¨æˆåŠŸç‡={current_success_rate:.2f} | å¹³å‡é™åˆ†={avg_drop:.2f}")
            
            writer.add_scalar('Main/Success_Rate_MA50', current_success_rate, episode)      # æœ€è¿‘ 50 ä¸ªå›åˆä¸­ï¼ŒæˆåŠŸç»•è¿‡æ£€æµ‹ï¼ˆåˆ†æ•° < 0.4ï¼‰çš„æ¯”ä¾‹ã€‚
            writer.add_scalar('Main/Similarity_Drop_MA50', avg_drop, episode)               # æœ€è¿‘ 50 ä¸ªå›åˆä¸­ï¼Œå¹³å‡æŠŠç›¸ä¼¼åº¦é™ä½äº†å¤šå°‘ï¼ˆåˆå§‹åˆ† 1.0 - æœ€ç»ˆåˆ†ï¼‰
            writer.add_scalar('Main/Episode_Reward', episode_reward, episode)               # Agent åœ¨ä¸€ä¸ªå›åˆå†…æ‹¿åˆ°çš„æ‰€æœ‰å¥–åŠ±ä¹‹å’Œã€‚
            writer.add_scalar('Main/Episode_Length', step + 1, episode)                      # ä¸€ä¸ªå›åˆå†…æ€»å…±æ‰§è¡Œäº†å¤šå°‘æ­¥ã€‚
            writer.add_histogram('Debug/Action_Distribution', np.array(episode_actions), episode)   # åœ¨å½“å‰å›åˆä¸­ï¼ŒAgent é€‰æ‹©äº†å“ªäº›å˜å¼‚åŠ¨ä½œï¼ˆAction 0-5ï¼‰ã€‚
            writer.add_scalar('Debug/Policy_Loss', loss, episode)                           # PPO ç®—æ³•æ›´æ–°æ—¶çš„ Loss å€¼ã€‚
            
            # å†™æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'a') as f:
                f.write(f"{episode},{step+1},{episode_reward:.4f},{loss:.4f},{current_success_rate:.2f}\n")
            
            # ä¿å­˜æ¨¡å‹
            if (episode + 1) % args.save_interval == 0:
                agent.save(os.path.join(args.model_dir, f'ppo_model_ep{episode+1}.pt'))
            
            if 'score' in info and info['score'] < best_score:
                best_score = info['score']
                agent.save(os.path.join(args.model_dir, 'ppo_model_best.pt'))

            # å®šæœŸæ¸…ç†
            if episode % 40 == 0:   
                cleanup_intermediate_files(args.save_path, episode_binaries)
    
    except KeyboardInterrupt:
        logger.warning("è®­ç»ƒä¸­æ–­")
    
    finally:
        agent.save(os.path.join(args.model_dir, 'ppo_model_final.pt'))
        writer.close()
        
        # ã€ä¼˜åŒ–ã€‘å–æ¶ˆæ³¨é‡Šï¼Œç¡®ä¿é€€å‡ºæ—¶æ¸…ç†åƒåœ¾
        cleanup_intermediate_files(args.save_path, episode_binaries)
        
        # ä¿å­˜æ¸…å•
        manifest_path = os.path.join(args.model_dir, 'episode_binaries.txt')
        with open(manifest_path, 'w') as f:
            for item in episode_binaries:
                f.write(f"{item['episode']},{item['step']},{item['binary']},{item['score']:.4f}\n")
        logger.info(f"âœ“ è®­ç»ƒç»“æŸï¼Œæ•°æ®å·²ä¿å­˜")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', required=True)
    parser.add_argument('--save-path', required=True)
    parser.add_argument('--state-dim', type=int, default=256) # é»˜è®¤256ç»´
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--gamma', type=float, default=0.95)
    parser.add_argument('--epsilon', type=float, default=0.2)
    parser.add_argument('--episodes', type=int, default=2000)
    parser.add_argument('--max-steps', type=int, default=30)
    parser.add_argument('--save-interval', type=int, default=5)
    parser.add_argument('--sample-hold-interval', type=int, default=10)
    parser.add_argument('--model-dir', default='./rl_models')
    parser.add_argument('--resume', default=None)
    parser.add_argument('--use-gpu', action='store_true')
    
    args = parser.parse_args()
    
    # æ¸…ç†æ—§æ—¥å¿—
    log_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'log/uroboro.log')
    if os.path.exists(log_path): os.remove(log_path)
        
    train_ppo(args)