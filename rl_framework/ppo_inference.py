#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Inference Script - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡ŒäºŒè¿›åˆ¶ä»£ç å˜å¼‚
"""

import os
import sys
import glob
import shutil
import json
import pickle
import numpy as np
import torch
import argparse
from loguru import logger
from tqdm import tqdm
import random   

# å¯¼å…¥ç¯å¢ƒå’Œ Agent
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env_wrapper import BinaryPerturbationEnv
from ppo_agent import PPOAgent
from run_utils import run_one

_INFERENCE_LOGGER_READY = False


def _setup_inference_logging():
    global _INFERENCE_LOGGER_READY
    if _INFERENCE_LOGGER_READY:
        return
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "log")
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, "ppo_inference.log")
    def _inference_log_filter(record):
        return record["name"] in ("ppo_inference", "__main__")
    logger.add(
        log_path,
        level="INFO",
        filter=_inference_log_filter,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    )
    logger.info(f"PPO inference log file: {log_path}")
    _INFERENCE_LOGGER_READY = True


def cleanup_inference_files(save_path, keep_binary):
    """
    æ¸…ç†æ¨ç†ä¸­é—´æ–‡ä»¶ï¼Œä»…ä¿ç•™æœ€ä½³ç»“æœ
    
    ä¿ç•™: inference_log.txt, æœ€ä½³å˜å¼‚ç»“æœç›®å½•
    åˆ é™¤: æ‰€æœ‰å…¶ä»– *_container ç›®å½•å’Œä¸´æ—¶æ–‡ä»¶
    """
    if not os.path.exists(save_path) or not keep_binary:
        return
    
    keep_path = os.path.abspath(keep_binary)
    keep_container = None
    
    # æ‰¾åˆ°éœ€è¦ä¿ç•™çš„containerç›®å½•
    if '_container' in keep_path:
        # keep_path å¯èƒ½æ˜¯ /path/xxx_container/xxx æˆ– /path/xxx_container
        parts = keep_path.split('_container')
        if parts:
            keep_container = parts[0] + '_container'
    
    deleted = 0
    freed = 0
    
    # æ¸…ç†æ‰€æœ‰containerç›®å½•ï¼ˆé™¤äº†éœ€è¦ä¿ç•™çš„ï¼‰
    for container in glob.glob(os.path.join(save_path, '*_container')):
        container_abs = os.path.abspath(container)
        
        # ä¿ç•™æœ€ä½³ç»“æœæ‰€åœ¨çš„container
        if keep_container and container_abs == keep_container:
            continue
        
        try:
            size = sum(os.path.getsize(os.path.join(d, f)) 
                      for d, _, files in os.walk(container) for f in files)
            shutil.rmtree(container)
            deleted += 1
            freed += size
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ é™¤ {os.path.basename(container)}: {e}")
    
    # æ¸…ç†å…¶ä»–ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™ inference_log.txtï¼‰
    for item in os.listdir(save_path):
        if item == 'inference_log.txt':
            continue
        
        path = os.path.join(save_path, item)
        if os.path.isfile(path):
            try:
                freed += os.path.getsize(path)
                os.remove(path)
                deleted += 1
            except Exception as e:
                logger.warning(f"æ— æ³•åˆ é™¤ {item}: {e}")
    
    # æ¸…ç† rl_output ä¸­çš„ä¸­é—´æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨ save_path ä¸‹çš„ç§æœ‰ç›®å½•ï¼‰
    rl_output = os.path.join(save_path, 'rl_output')
    if not os.path.exists(rl_output):
        rl_output = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'rl_output')
    if os.path.exists(rl_output):
        for mutant in glob.glob(os.path.join(rl_output, 'mutant_*.bin*')):
            try:
                freed += os.path.getsize(mutant)
                os.remove(mutant)
                deleted += 1
            except Exception as e:
                logger.warning(f"æ— æ³•åˆ é™¤ {os.path.basename(mutant)}: {e}")
    
    if deleted > 0:
        size_mb = freed / (1024 * 1024)
        logger.success(f"âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted} é¡¹ï¼Œé‡Šæ”¾ {size_mb:.2f} MB")
    
    if keep_container:
        logger.info(f"âœ“ å·²ä¿ç•™æœ€ä½³ç»“æœ: {os.path.basename(keep_container)}")


def _find_sym_to_addr(binary_path):
    """Best-effort lookup for sym_to_addr.pickle near a binary."""
    base_dir = os.path.dirname(os.path.abspath(binary_path))
    candidates = [
        os.path.join(base_dir, "sym_to_addr.pickle"),
        os.path.join(os.path.dirname(base_dir), "sym_to_addr.pickle"),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path
    return None


def _load_sym_to_addr(path):
    if not path or not os.path.exists(path):
        return {}
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except Exception:
        return {}


def _resolve_func_addr(binary_path, func_name):
    sym_path = _find_sym_to_addr(binary_path)
    sym_map = _load_sym_to_addr(sym_path)
    return sym_map.get(func_name)


def inference_ppo(args):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ PPO æ¨¡å‹è¿›è¡Œæ¨ç†
    
    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°
            - model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            - binary: åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
            - function: ç›®æ ‡å‡½æ•°å
            - save_path: å˜å¼‚ç»“æœä¿å­˜è·¯å¾„
            - max_steps: æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤30ï¼‰
            - target_score: ç›®æ ‡åˆ†æ•°ï¼ˆé»˜è®¤0.40ï¼‰
    """
    logger.info("=" * 80)
    logger.info("PPO æ¨ç†æ¨¡å¼")
    logger.info("=" * 80)
    logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"åŸå§‹äºŒè¿›åˆ¶: {args.binary}")
    logger.info(f"ç›®æ ‡å‡½æ•°: {args.function}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    logger.info(f"æœ€å¤§æ­¥æ•°: {args.max_steps}")
    logger.info(f"ç›®æ ‡åˆ†æ•°: {args.target_score}")
    logger.info(f"æ£€æµ‹æ–¹æ³•: {args.detection_method}")
    _setup_inference_logging()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_path, exist_ok=True)
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    logger.info("åˆå§‹åŒ–å˜å¼‚ç¯å¢ƒ...")
    dataset_path = os.path.join(args.save_path, "inference_dataset.json")
    dataset = [{
        "binary_path": os.path.abspath(args.binary),
        "binary_name": os.path.basename(args.binary),
        "version": "inference",
        "opt_level": "unknown",
        "func_name": args.function,
        "func_addr": 0,
        "size": 0,
        "id": "inference"
    }]
    with open(dataset_path, "w") as f:
        json.dump(dataset, f, indent=2)

    env = BinaryPerturbationEnv(
        save_path=args.save_path,
        dataset_path=dataset_path,
        sample_hold_interval=1,
        max_steps=args.max_steps
    )
    env.set_state_dim(args.state_dim)
    env.target_score = args.target_score
    logger.info("ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ âœ“")
    
    # åˆå§‹åŒ– PPO Agent å¹¶åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    device = 'cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    agent = PPOAgent(
        state_dim=args.state_dim,
        n_actions=env.n_actions,
        action_map=list(env.action_ids),
        device=device
    )
    agent.load(args.model_path)
    agent.policy.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ âœ“ (è®¾å¤‡: {device})")
    
    # æ‰§è¡Œæ¨ç†
    logger.info("=" * 80)
    logger.info("å¼€å§‹å˜å¼‚è¿‡ç¨‹")
    logger.info("=" * 80)
    
    state = env.reset()
    success = False
    best_score = float('inf')
    best_binary = None
    asm_work_dir = os.path.join(args.save_path, "_asm2vec_eval")
    os.makedirs(asm_work_dir, exist_ok=True)
    original_asm_cache = {}
    
    # è®°å½•æ¯æ­¥ä¿¡æ¯
    step_records = []
    
    for step in range(args.max_steps):
        logger.info(f"\næ­¥éª¤ {step + 1}/{args.max_steps}")
        logger.info("-" * 60)
        
        # ä½¿ç”¨æ¨¡å‹é€‰æ‹©åŠ¨ä½œï¼ˆä¸æ¢ç´¢ï¼Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼‰
        joint_idx, loc_idx, act_idx, actual_action, log_prob, value = agent.select_action(state, explore=False)

        logger.info(f"é€‰æ‹©åŠ¨ä½œ: {actual_action} (ä½ç½®: {loc_idx}, åŠ¨ä½œç´¢å¼•: {act_idx}, è”åˆç´¢å¼•: {joint_idx})")
        logger.info(f"çŠ¶æ€ä»·å€¼: {value:.4f}")
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, info = env.step(actual_action, loc_idx)
        
        # å¯é€‰ï¼šä½¿ç”¨æŒ‡å®šæ£€æµ‹æ–¹æ³•é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦
        eval_score = info.get('score', 1.0)
        if info.get('binary') and args.detection_method != "asm2vec":
            mutated_func_addr = None
            sym_to_addr_path = _find_sym_to_addr(info.get('binary'))
            if args.detection_method == "safe":
                mutated_func_addr = _resolve_func_addr(info.get('binary'), env.function_name)
                sym_to_addr_path = _find_sym_to_addr(env.original_binary)
            eval_score, _ = run_one(
                env.original_binary,
                info.get('binary'),
                model_original=None,
                checkdict={},
                function_name=env.function_name,
                detection_method=args.detection_method,
                asm_work_dir=asm_work_dir,
                original_asm_cache=original_asm_cache,
                simple_mode=True,
                original_func_addr=dataset[0].get("func_addr"),
                mutated_func_addr=mutated_func_addr,
                sym_to_addr_path=sym_to_addr_path,
                safe_checkpoint_dir=args.safe_checkpoint_dir,
                safe_i2v_dir=args.safe_i2v_dir,
                safe_use_gpu=args.safe_use_gpu,
            )
            if eval_score is None:
                eval_score = info.get('score', 1.0)
        logger.info(
            f"Step {step+1}: action={actual_action}, loc={loc_idx}, reward={reward:.4f}, eval_score={eval_score}"
            f"env_score={info.get('score')}, eval_score={eval_score}, done={done}, "
            f"should_reset={info.get('should_reset')}"
        )

        # è®°å½•ä¿¡æ¯
        step_info = {
            'step': step + 1,
            'loc': loc_idx,
            'act_idx': act_idx,
            'action': actual_action,
            'score': eval_score,
            'grad': info.get('grad', 0.0),
            'binary': info.get('binary', None),
            'reward': reward,
            'value': value
        }
        step_records.append(step_info)
        
        # è¾“å‡ºç»“æœ
        if eval_score is not None:
            logger.info(f"ç›¸ä¼¼åº¦åˆ†æ•°: {eval_score:.4f}")
            logger.info(f"æ¢¯åº¦å€¼: {info.get('grad', 0):.4f}")
            logger.info(f"å¥–åŠ±: {reward:.4f}")
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if eval_score < best_score:
                best_score = eval_score
                best_binary = info.get('binary', None)
                logger.success(f"âœ¨ å‘ç°æ›´å¥½çš„ç»“æœ! åˆ†æ•°: {best_score:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if eval_score < args.target_score:
                success = True
                logger.success(f"ğŸ‰ æˆåŠŸè¾¾åˆ°ç›®æ ‡! åˆ†æ•°: {eval_score:.4f} < {args.target_score}")
                logger.success(f"å˜å¼‚åçš„äºŒè¿›åˆ¶: {info.get('binary', 'unknown')}")
                break
        else:
            logger.warning("æœªèƒ½è·å–è¯„ä¼°åˆ†æ•°")
        
        state = next_state
        
        if args.detection_method == "asm2vec":
            if done:
                logger.info("å›åˆç»“æŸ")
                break
        else:
            if info.get("should_reset"):
                logger.info("å›åˆç»“æŸ (should_reset)")
                break
    
    # è¾“å‡ºæ€»ç»“
    logger.info("")
    logger.info("=" * 80)
    logger.info("æ¨ç†å®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"æ‰§è¡Œæ­¥æ•°: {step + 1}")
    logger.info(f"æœ€ä½³åˆ†æ•°: {best_score:.4f}")
    
    if success:
        logger.success(f"âœ“ æˆåŠŸè¾¾åˆ°ç›®æ ‡ (åˆ†æ•° < {args.target_score})")
    else:
        logger.warning(f"âœ— æœªè¾¾åˆ°ç›®æ ‡ (åˆ†æ•° >= {args.target_score})")
    
    if best_binary:
        logger.info(f"æœ€ä½³å˜å¼‚ç»“æœ: {best_binary}")
    logger.info(
        f"RESULT: success={int(success)}, steps={step + 1}, "
        f"best_score={best_score:.6f}, target_score={args.target_score}"
    )
    
    # ä¿å­˜æ¨ç†æ—¥å¿—
    log_file = os.path.join(args.save_path, 'inference_log.txt')
    with open(log_file, 'w') as f:
        f.write(f"æ¨¡å‹: {args.model_path}\n")
        f.write(f"äºŒè¿›åˆ¶: {args.binary}\n")
        f.write(f"å‡½æ•°: {args.function}\n")
        f.write(f"æœ€ä½³åˆ†æ•°: {best_score:.4f}\n")
        f.write(f"æˆåŠŸ: {success}\n")
        f.write(f"æœ€ä½³ç»“æœ: {best_binary}\n\n")
        f.write("æ­¥éª¤è¯¦æƒ…:\n")
        f.write("step,loc,act_idx,action,score,grad,reward,value,binary\n")
        for record in step_records:
            f.write(f"{record['step']},{record['loc']},{record['act_idx']},{record['action']},{record['score']:.4f},"
                   f"{record['grad']:.4f},{record['reward']:.4f},{record['value']:.4f},"
                   f"{record['binary']}\n")
    
    logger.info(f"æ¨ç†æ—¥å¿—å·²ä¿å­˜: {log_file}")
    
    # æ¸…ç†ä¸­é—´æ–‡ä»¶ï¼Œåªä¿ç•™æœ€ä½³ç»“æœ
    logger.info("")
    cleanup_inference_files(args.save_path, best_binary)
    
    return best_score, best_binary, success


def batch_inference(args):
    """
    æ‰¹é‡æ¨ç†ï¼šå¯¹å¤šä¸ªäºŒè¿›åˆ¶æ–‡ä»¶æˆ–å‡½æ•°è¿›è¡Œå˜å¼‚
    """
    logger.info("æ‰¹é‡æ¨ç†æ¨¡å¼")
    
    # è¯»å–æ‰¹é‡é…ç½®æ–‡ä»¶
    if not os.path.exists(args.batch_file):
        logger.error(f"æ‰¹é‡é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.batch_file}")
        return
    
    with open(args.batch_file, 'r') as f:
        lines = f.readlines()
    
    results = []
    
    for idx, line in enumerate(lines):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        
        parts = line.split(',')
        if len(parts) < 2:
            logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ: {line}")
            continue
        
        binary, function = parts[0], parts[1]
        save_path = parts[2] if len(parts) > 2 else f"{args.save_path}_{idx}"
        
        logger.info("")
        logger.info("=" * 80)
        logger.info(f"æ‰¹é‡ä»»åŠ¡ {idx + 1}/{len(lines)}")
        logger.info("=" * 80)
        
        # è®¾ç½®å‚æ•°
        args.binary = binary
        args.function = function
        args.save_path = save_path
        
        # æ‰§è¡Œæ¨ç†
        try:
            best_score, best_binary, success = inference_ppo(args)
            results.append({
                'binary': binary,
                'function': function,
                'score': best_score,
                'success': success,
                'output': best_binary
            })
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤±è´¥: {e}")
            results.append({
                'binary': binary,
                'function': function,
                'score': float('inf'),
                'success': False,
                'output': None
            })
    
    # è¾“å‡ºæ‰¹é‡ç»“æœ
    logger.info("")
    logger.info("=" * 80)
    logger.info("æ‰¹é‡æ¨ç†å®Œæˆ")
    logger.info("=" * 80)
    
    success_count = sum(1 for r in results if r['success'])
    logger.info(f"æ€»ä»»åŠ¡æ•°: {len(results)}")
    logger.info(f"æˆåŠŸæ•°: {success_count}")
    logger.info(f"æˆåŠŸç‡: {success_count / len(results) * 100:.2f}%")
    
    # ä¿å­˜æ‰¹é‡ç»“æœ
    batch_log = os.path.join(os.path.dirname(args.batch_file), 'batch_inference_results.txt')
    with open(batch_log, 'w') as f:
        f.write("binary,function,score,success,output\n")
        for r in results:
            f.write(f"{r['binary']},{r['function']},{r['score']:.4f},"
                   f"{r['success']},{r['output']}\n")
    
    logger.info(f"æ‰¹é‡ç»“æœå·²ä¿å­˜: {batch_log}")


def _pin_sample(env, sample_idx, sample):
    env.current_sample_idx = sample_idx
    env.current_sample_data = sample
    env.episodes_on_current = 0
    env.original_func_addr = None
    env.original_binary = sample.get("binary_path")
    env.function_name = sample.get("func_name")


def evaluate_dataset(args):
    logger.info("=" * 80)
    logger.info("PPO æ•°æ®é›†è¯„ä¼°æ¨¡å¼")
    logger.info("=" * 80)
    logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"æ•°æ®é›†: {args.dataset}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    logger.info(f"æœ€å¤§æ­¥æ•°: {args.max_steps}")
    logger.info(f"ç›®æ ‡åˆ†æ•°: {args.target_score}")
    logger.info(f"æ£€æµ‹æ–¹æ³•: {args.detection_method}")
    _setup_inference_logging()

    if not os.path.exists(args.model_path):
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.dataset):
        logger.error(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset}")
        return

    with open(args.dataset, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    if not isinstance(dataset, list):
        logger.error("æ•°æ®é›†æ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯ JSON åˆ—è¡¨")
        return

    if args.seed is not None:
        random.seed(args.seed)
    random.shuffle(dataset)

    if args.limit is not None:
        dataset = dataset[: args.limit]

    os.makedirs(args.save_path, exist_ok=True)

    env = BinaryPerturbationEnv(
        save_path=args.save_path,
        dataset_path=args.dataset,
        sample_hold_interval=10**9,
        max_steps=args.max_steps,
    )
    env.set_state_dim(args.state_dim)
    env.target_score = args.target_score

    device = 'cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    agent = PPOAgent(
        state_dim=args.state_dim,
        n_actions=env.n_actions,
        action_map=list(env.action_ids),
        device=device
    )
    agent.load(args.model_path)
    agent.policy.eval()

    asm_work_dir = os.path.join(args.save_path, "_asm2vec_eval")
    os.makedirs(asm_work_dir, exist_ok=True)
    original_asm_cache = {}

    success_count = 0
    total = len(dataset)

    pbar = tqdm(enumerate(dataset), total=total, desc="PPO Eval", unit="sample")
    for idx, sample in pbar:
        _pin_sample(env, idx, sample)
        state = env.reset(force_switch=False)

        best_score = 1.0
        success = False

        for step in range(args.max_steps):
            joint_idx, loc_idx, act_idx, actual_action, log_prob, value = agent.select_action(state, explore=True)
            next_state, reward, done, info = env.step(actual_action, loc_idx)
            print(f"Step {step}: action={actual_action}, loc={loc_idx}")
            eval_score = info.get('score', 1.0)
            if info.get('binary') and args.detection_method != "asm2vec":
                mutated_func_addr = None
                sym_to_addr_path = _find_sym_to_addr(info.get('binary'))
                if args.detection_method == "safe":
                    mutated_func_addr = _resolve_func_addr(info.get('binary'), env.function_name)
                    sym_to_addr_path = _find_sym_to_addr(env.original_binary)
                eval_score, _ = run_one(
                    env.original_binary,
                    info.get('binary'),
                    model_original=None,
                    checkdict={},
                    function_name=env.function_name,
                    detection_method=args.detection_method,
                    asm_work_dir=asm_work_dir,
                    original_asm_cache=original_asm_cache,
                    simple_mode=True,
                    original_func_addr=sample.get("func_addr"),
                    mutated_func_addr=mutated_func_addr,
                    sym_to_addr_path=sym_to_addr_path,
                    safe_checkpoint_dir=args.safe_checkpoint_dir,
                    safe_i2v_dir=args.safe_i2v_dir,
                    safe_use_gpu=args.safe_use_gpu,
                )
                if eval_score is None:
                    eval_score = info.get('score', 1.0)

            logger.info(
                f"Eval step {step+1}: action={actual_action}, loc={loc_idx}, reward={reward:.4f}, "
                f"env_score={info.get('score')}, eval_score={eval_score}, done={done}, "
                f"should_reset={info.get('should_reset')}"
            )
            if eval_score is not None and eval_score < best_score:
                best_score = eval_score
                if eval_score < args.target_score:
                    success = True
                    break

            state = next_state
            if args.detection_method == "asm2vec":
                if done:
                    break
            else:
                if info.get("should_reset"):
                    break

        if success:
            success_count += 1

        if (idx + 1) % 10 == 0:
            pbar.set_postfix({"success_rate": f"{success_count/max(1, idx+1):.3f}"})

    success_rate = success_count / max(1, total)
    logger.success(f"âœ“ æµ‹è¯•å®Œæˆ: success_rate={success_rate:.4f} ({success_count}/{total})")
    logger.info(
        f"RESULT: success_rate={success_rate:.6f}, success_count={success_count}, total={total}, "
        f"target_score={args.target_score}"
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PPO Inference for Binary Perturbation')
    
    # æ¨¡å‹å‚æ•°
    default_model = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'rl_models/ppo_model_ep200.pt')
    parser.add_argument('--model-path', default=default_model, help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    
    # ç›®æ ‡å‚æ•°
    default_dataset = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'datasets/fast_0128/dataset_test.json')
    parser.add_argument('--binary', help='åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--function', help='ç›®æ ‡å‡½æ•°å')
    parser.add_argument('--save-path', help='å˜å¼‚ç»“æœä¿å­˜è·¯å¾„')
    parser.add_argument('--dataset', default=default_dataset, help='æµ‹è¯•æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡')
    parser.add_argument('--eval-dataset', action='store_true', help='è¯„ä¼°æ•´ä¸ªæ•°æ®é›†çš„æˆåŠŸç‡')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--state-dim', type=int, default=256, help='çŠ¶æ€ç»´åº¦ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œé»˜è®¤ 256ï¼‰')
    parser.add_argument('--max-steps', type=int, default=30, help='æœ€å¤§æ­¥æ•°')
    parser.add_argument('--target-score', type=float, default=0.40, help='ç›®æ ‡ç›¸ä¼¼åº¦åˆ†æ•°')
    parser.add_argument('--detection-method', choices=['asm2vec', 'safe'], default='asm2vec', help='ç›¸ä¼¼åº¦æ£€æµ‹æ–¹æ³•')
    parser.add_argument('--use-gpu', action='store_true', help='ä½¿ç”¨ GPU')
    parser.add_argument('--safe-checkpoint-dir', default=None, help='SAFE æ¨¡å‹ checkpoint ç›®å½•')
    parser.add_argument('--safe-i2v-dir', default=None, help='SAFE i2v ç›®å½•')
    parser.add_argument('--safe-use-gpu', action='store_true', help='SAFE ä½¿ç”¨ GPU')
    
    # æ‰¹é‡æ¨¡å¼
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡æ¨ç†æ¨¡å¼')
    parser.add_argument('--batch-file', help='æ‰¹é‡é…ç½®æ–‡ä»¶ (æ ¼å¼: binary,function,save_path)')
    
    args = parser.parse_args()
    
    # æ‰¹é‡æ¨¡å¼/æ•°æ®é›†è¯„ä¼°/å•æ¬¡æ¨ç†
    if args.eval_dataset:
        if not args.save_path:
            logger.error("æ•°æ®é›†è¯„ä¼°éœ€è¦æŒ‡å®š --save-path")
            sys.exit(1)
        evaluate_dataset(args)
    elif args.batch:
        if not args.batch_file:
            logger.error("æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --batch-file")
            sys.exit(1)
        batch_inference(args)
    else:
        if not args.binary or not args.function or not args.save_path:
            logger.error("å•æ¬¡æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --binary, --function, --save-path")
            sys.exit(1)
        inference_ppo(args)
