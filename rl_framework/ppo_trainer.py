#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Trainer for Binary Code Perturbation
PPO è®­ç»ƒå™¨ï¼ˆç›´æ¥è°ƒç”¨ç¯å¢ƒï¼‰
"""
import os
import csv
import json
import numpy as np
import torch
from ppo_agent import PPOAgent
import argparse
from loguru import logger
import sys
import shutil
import glob
import random
import pickle
from collections import deque
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from typing import Dict, List, Optional

# å¯¼å…¥ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env_wrapper import BinaryPerturbationEnv
from run_utils import run_one


def cleanup_intermediate_files(save_path, episode_binaries=None):
    """
    æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ–‡ä»¶
    """
    if not os.path.exists(save_path):
        return
    
    logger.info(f"å¼€å§‹æ¸…ç†ä¸­é—´æ–‡ä»¶: {save_path}")
    
    # æå–éœ€è¦ä¿ç•™çš„äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
    binaries_to_keep = set()
    if episode_binaries:
        for item in episode_binaries:
            if 'binary' in item and item['binary']:
                binaries_to_keep.add(os.path.abspath(item['binary']))
        logger.info(f"å°†ä¿ç•™ {len(binaries_to_keep)} ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶")
    
    deleted_count = 0
    deleted_size = 0
    
    # æ¸…ç† tmp_* ä¸´æ—¶ç›®å½•
    tmp_pattern = os.path.join(save_path, 'tmp_*')
    for tmp_dir in glob.glob(tmp_pattern):
        if os.path.isdir(tmp_dir):
            try:
                size = sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, _, filenames in os.walk(tmp_dir) for filename in filenames)
                shutil.rmtree(tmp_dir)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½• {tmp_dir}: {e}")
    
    # æ¸…ç† *_container å®¹å™¨ç›®å½•
    container_pattern = os.path.join(save_path, '*_container')
    for container_dir in glob.glob(container_pattern):
        if os.path.isdir(container_dir):
            container_dir_abs = os.path.abspath(container_dir)
            should_keep = False
            
            if binaries_to_keep:
                for binary_path in binaries_to_keep:
                    if binary_path.startswith(container_dir_abs + os.sep) or binary_path == container_dir_abs:
                        should_keep = True
                        break
            
            if should_keep: continue
            
            try:
                size = sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, _, filenames in os.walk(container_dir) for filename in filenames)
                shutil.rmtree(container_dir)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤å®¹å™¨ç›®å½• {container_dir}: {e}")
    
    # æ¸…ç† rl_output ä¸­çš„ä¸­é—´æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨ save_path ä¸‹çš„ç§æœ‰ç›®å½•ï¼‰
    rl_output_dir = os.path.join(save_path, 'rl_output')
    if not os.path.exists(rl_output_dir):
        current_dir = os.path.dirname(os.path.abspath(__file__))
        rl_output_dir = os.path.join(current_dir, 'rl_output')
    if os.path.exists(rl_output_dir):
        mutant_files = glob.glob(os.path.join(rl_output_dir, 'mutant_*.bin*'))
        for file_path in mutant_files:
            try:
                size = os.path.getsize(file_path)
                os.remove(file_path)
                deleted_count += 1
                deleted_size += size
            except Exception as e:
                pass
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    if deleted_size < 1024: size_str = f"{deleted_size} B"
    elif deleted_size < 1024**2: size_str = f"{deleted_size/1024:.2f} KB"
    else: size_str = f"{deleted_size/1024**2:.2f} MB"
    
    logger.success(f"âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªé¡¹ç›®ï¼Œé‡Šæ”¾ {size_str} ç©ºé—´")


def _load_trainer_state(path):
    if not path or not os.path.exists(path):
        return None
    try:
        checkpoint = torch.load(path, map_location="cpu")
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–è®­ç»ƒçŠ¶æ€: {path}, {e}")
        return None
    return checkpoint.get("trainer_state")


def _find_sym_to_addr(binary_path: str) -> Optional[str]:
    base_dir = os.path.dirname(os.path.abspath(binary_path))
    candidates = [
        os.path.join(base_dir, "sym_to_addr.pickle"),
        os.path.join(os.path.dirname(base_dir), "sym_to_addr.pickle"),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path
    return None


def _load_sym_to_addr(path: Optional[str]) -> Dict:
    if not path or not os.path.exists(path):
        return {}
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except Exception:
        return {}


def _parse_addr(value) -> Optional[int]:
    if value is None:
        return None
    try:
        if isinstance(value, str):
            return int(value, 16) if value.startswith(("0x", "0X")) else int(value)
        return int(value)
    except Exception:
        return None


def _opt_rank(opt_level: str) -> int:
    order = {"O0": 0, "O1": 1, "O2": 2, "O3": 3, "Os": 4, "Oz": 5}
    return order.get(str(opt_level), 99)


class TargetedAttackEvaluator:
    """
    å®šå‘æ”»å‡»è¯„æµ‹ï¼š
    - attacker å‡½æ•°: éšæœºæ ·æœ¬
    - target èº«ä»½: éšæœºå¦ä¸€å‡½æ•°èº«ä»½çš„å¤šä¸ªç¼–è¯‘å˜ä½“
    - ç›®æ ‡: æœ€å¤§åŒ– min(sim(attacker_adv, v_i))
    """

    def __init__(self, args):
        self.args = args
        self.dataset_path = args.targeted_eval_dataset or args.dataset
        self.eval_steps = max(1, int(args.targeted_eval_max_steps))
        self.pairs = max(1, int(args.targeted_eval_pairs))
        self.max_target_variants = max(1, int(args.targeted_eval_max_target_variants))
        self.min_target_variants = max(1, int(args.targeted_eval_min_target_variants))
        self.success_threshold = float(args.targeted_eval_threshold)
        self.rng = random.Random(args.targeted_eval_seed)

        with open(self.dataset_path, "r") as f:
            self.dataset = json.load(f)
        if not isinstance(self.dataset, list) or not self.dataset:
            raise ValueError("targeted eval dataset is empty or invalid")

        self.id_to_item = {}
        self.id_to_index = {}
        for idx, item in enumerate(self.dataset):
            sid = str(item.get("id", "")).strip()
            if not sid:
                continue
            self.id_to_item[sid] = item
            self.id_to_index[sid] = idx

        eval_save_path = os.path.join(args.save_path, "targeted_eval")
        os.makedirs(eval_save_path, exist_ok=True)
        self.env = BinaryPerturbationEnv(
            save_path=eval_save_path,
            dataset_path=self.dataset_path,
            sample_hold_interval=10**9,
            max_steps=self.eval_steps,
            detection_method=args.detection_method,
            safe_checkpoint_dir=args.safe_checkpoint_dir,
            safe_i2v_dir=args.safe_i2v_dir,
            safe_use_gpu=args.safe_use_gpu,
            safe_cache_enabled=(args.detection_method == "safe" and not args.no_safe_cache),
            jtrans_model_dir=args.jtrans_model_dir,
            jtrans_tokenizer_dir=args.jtrans_tokenizer_dir,
            jtrans_use_gpu=args.jtrans_use_gpu,
            feature_mode=args.feature_mode,
            seed=args.seed,
            stall_limit=args.stall_limit,
            progress_eps=args.progress_eps,
            progress_reward_eps=args.progress_reward_eps,
            include_schedule_feature=args.include_schedule_feature,
            strict_invalid_loc=(not args.non_strict_invalid_loc),
            hold_min=args.hold_min,
            hold_max=args.hold_max,
        )
        self.env.set_state_dim(args.state_dim)
        # å®šå‘è¯„æµ‹åªå— max_steps çº¦æŸï¼Œä¸å¸Œæœ›è¢« env.target_score æå‰ doneã€‚
        self.env.target_score = -1.0

        self._target_original_asm_cache = {}
        self._mutated_asm_cache = {}
        self._mutated_sym_cache = {}

    def _pin_sample(self, sample_idx: int) -> None:
        sample = self.env.dataset[sample_idx]
        self.env.current_sample_idx = sample_idx
        self.env.current_sample_data = sample
        self.env.episodes_on_current = 0
        self.env.original_func_addr = None
        self.env.original_binary = sample["binary_path"]
        self.env.function_name = sample["func_name"]

    def _collect_target_variants(self, target_item: Dict) -> List[Dict]:
        target_ids = []
        anchor_id = str(target_item.get("id", "")).strip()
        if anchor_id:
            target_ids.append(anchor_id)
        for vid in target_item.get("variants", []) or []:
            svid = str(vid).strip()
            if svid:
                target_ids.append(svid)

        dedup = []
        seen = set()
        for tid in target_ids:
            if tid in seen:
                continue
            seen.add(tid)
            item = self.id_to_item.get(tid)
            if item is not None:
                dedup.append(item)

        dedup.sort(
            key=lambda x: (
                _opt_rank(x.get("opt_level")),
                str(x.get("version", "")),
                str(x.get("binary_name", "")),
            )
        )
        return dedup[: self.max_target_variants]

    def _sample_pairs(self) -> List[Dict]:
        target_candidates = []
        for item in self.dataset:
            variants = self._collect_target_variants(item)
            if len(variants) >= self.min_target_variants:
                t_ids = {str(v.get("id")) for v in variants if v.get("id")}
                target_candidates.append((item, variants, t_ids))

        if not target_candidates:
            return []

        pairs = []
        seen = set()
        max_attempts = self.pairs * 40
        attempts = 0
        while len(pairs) < self.pairs and attempts < max_attempts:
            attempts += 1
            target_item, target_variants, target_id_set = self.rng.choice(target_candidates)
            target_func = str(target_item.get("func_name", ""))
            attacker_idx = self.rng.randrange(len(self.dataset))
            attacker_item = self.dataset[attacker_idx]
            attacker_id = str(attacker_item.get("id", ""))
            attacker_func = str(attacker_item.get("func_name", ""))
            if not attacker_id or attacker_id in target_id_set:
                continue
            if attacker_func == target_func:
                continue
            key = (attacker_id, str(target_item.get("id", "")))
            if key in seen:
                continue
            seen.add(key)
            pairs.append(
                {
                    "attacker_idx": attacker_idx,
                    "attacker": attacker_item,
                    "target": target_item,
                    "target_variants": target_variants,
                }
            )
        return pairs

    def _resolve_mutated_addr(self, mutated_binary: str, attacker_func_name: str) -> Optional[int]:
        bpath = os.path.abspath(mutated_binary)
        sym_map = self._mutated_sym_cache.get(bpath)
        if sym_map is None:
            sym_map = _load_sym_to_addr(_find_sym_to_addr(bpath))
            self._mutated_sym_cache[bpath] = sym_map
        if not sym_map:
            return None
        return _parse_addr(sym_map.get(attacker_func_name))

    def _score_binary_against_target_variants(
        self, mutated_binary: str, attacker_func_name: str, target_variants: List[Dict]
    ) -> Dict:
        attacker_addr = self._resolve_mutated_addr(mutated_binary, attacker_func_name)
        if attacker_addr is None:
            return {"valid": False, "scores": [], "min_score": -1.0, "avg_score": -1.0}

        scores = []
        for tv in target_variants:
            target_binary = tv.get("binary_path")
            target_func = str(tv.get("func_name", ""))
            target_addr = _parse_addr(tv.get("func_addr"))
            if not target_binary or not target_func or target_addr is None:
                continue

            score, _grad = run_one(
                original_binary=target_binary,
                mutated_binary=mutated_binary,
                model_original=None,
                checkdict={},
                function_name=target_func,
                detection_method=self.args.detection_method,
                asm_work_dir=self.env._asm_work_dir,
                original_asm_cache=self._target_original_asm_cache,
                simple_mode=True,
                original_func_addr=target_addr,
                mutated_func_addr=attacker_addr,
                safe_checkpoint_dir=self.args.safe_checkpoint_dir,
                safe_i2v_dir=self.args.safe_i2v_dir,
                safe_use_gpu=self.args.safe_use_gpu,
                mutated_asm_cache=self._mutated_asm_cache,
                safe_cache=self.env.safe_cache,
                jtrans_model_dir=self.args.jtrans_model_dir,
                jtrans_tokenizer_dir=self.args.jtrans_tokenizer_dir,
                jtrans_use_gpu=self.args.jtrans_use_gpu,
                jtrans_cache=self.env._jtrans_cache,
            )
            if score is None:
                continue
            scores.append(float(score))

        if len(scores) != len(target_variants):
            return {"valid": False, "scores": scores, "min_score": -1.0, "avg_score": -1.0}
        return {
            "valid": True,
            "scores": scores,
            "min_score": float(min(scores)),
            "avg_score": float(sum(scores) / len(scores)),
        }

    def _run_one_pair(self, agent: PPOAgent, pair: Dict) -> Dict:
        attacker = pair["attacker"]
        target = pair["target"]
        target_variants = pair["target_variants"]
        self._pin_sample(pair["attacker_idx"])
        state = self.env.reset(force_switch=False)

        pre_eval = self._score_binary_against_target_variants(
            self.env.current_binary,
            str(attacker.get("func_name", "")),
            target_variants,
        )
        best_eval = dict(pre_eval)
        best_binary = self.env.current_binary
        best_step = 0
        err = ""
        steps_used = 0

        for step in range(self.eval_steps):
            loc_mask = self.env.get_loc_mask(self.args.n_locs)
            (
                _joint_idx,
                loc_idx,
                _act_idx,
                actual_action,
                _log_prob,
                _value,
            ) = agent.select_action(state, explore=self.args.targeted_eval_explore, loc_mask=loc_mask)
            next_state, _reward, done, info = self.env.step(actual_action, loc_idx)
            steps_used = step + 1

            cand_binary = info.get("binary")
            if cand_binary:
                cur_eval = self._score_binary_against_target_variants(
                    cand_binary,
                    str(attacker.get("func_name", "")),
                    target_variants,
                )
                if cur_eval["valid"] and (not best_eval["valid"] or cur_eval["min_score"] > best_eval["min_score"]):
                    best_eval = cur_eval
                    best_binary = cand_binary
                    best_step = steps_used

            if info.get("should_reset"):
                err = str(info.get("error", "should_reset"))
                break

            state = next_state
            if done:
                break

        pre_min = pre_eval["min_score"] if pre_eval["valid"] else -1.0
        post_min = best_eval["min_score"] if best_eval["valid"] else -1.0
        pre_avg = pre_eval["avg_score"] if pre_eval["valid"] else -1.0
        post_avg = best_eval["avg_score"] if best_eval["valid"] else -1.0

        return {
            "attacker_id": str(attacker.get("id", "")),
            "attacker_func": str(attacker.get("func_name", "")),
            "target_id": str(target.get("id", "")),
            "target_func": str(target.get("func_name", "")),
            "target_variants": len(target_variants),
            "steps_used": steps_used,
            "best_step": best_step,
            "pre_valid": int(pre_eval["valid"]),
            "post_valid": int(best_eval["valid"]),
            "pre_min": pre_min,
            "post_min": post_min,
            "pre_avg": pre_avg,
            "post_avg": post_avg,
            "gain_min": post_min - pre_min,
            "gain_avg": post_avg - pre_avg,
            "success_pre": int(pre_min >= self.success_threshold),
            "success_post": int(post_min >= self.success_threshold),
            "improved": int(post_min > pre_min),
            "error": err,
            "best_binary": best_binary or "",
        }

    def evaluate(self, agent: PPOAgent, episode: int) -> Optional[Dict]:
        pairs = self._sample_pairs()
        if not pairs:
            logger.warning("TargetedEval: æ— æ³•é‡‡æ ·åˆ°æœ‰æ•ˆ attacker-target é…å¯¹ï¼Œè·³è¿‡")
            return None

        rows = []
        for pair in pairs:
            try:
                rows.append(self._run_one_pair(agent, pair))
            except Exception as e:
                rows.append(
                    {
                        "attacker_id": str(pair["attacker"].get("id", "")),
                        "attacker_func": str(pair["attacker"].get("func_name", "")),
                        "target_id": str(pair["target"].get("id", "")),
                        "target_func": str(pair["target"].get("func_name", "")),
                        "target_variants": len(pair["target_variants"]),
                        "steps_used": 0,
                        "best_step": 0,
                        "pre_valid": 0,
                        "post_valid": 0,
                        "pre_min": -1.0,
                        "post_min": -1.0,
                        "pre_avg": -1.0,
                        "post_avg": -1.0,
                        "gain_min": 0.0,
                        "gain_avg": 0.0,
                        "success_pre": 0,
                        "success_post": 0,
                        "improved": 0,
                        "error": str(e),
                        "best_binary": "",
                    }
                )

        valid_rows = [r for r in rows if r["pre_valid"] and r["post_valid"]]
        metrics = {"pairs_total": len(rows), "pairs_valid": len(valid_rows)}
        if valid_rows:
            denom = float(len(valid_rows))
            metrics.update(
                {
                    "min_pre": float(sum(r["pre_min"] for r in valid_rows) / denom),
                    "min_post": float(sum(r["post_min"] for r in valid_rows) / denom),
                    "avg_pre": float(sum(r["pre_avg"] for r in valid_rows) / denom),
                    "avg_post": float(sum(r["post_avg"] for r in valid_rows) / denom),
                    "gain_min": float(sum(r["gain_min"] for r in valid_rows) / denom),
                    "gain_avg": float(sum(r["gain_avg"] for r in valid_rows) / denom),
                    "success_pre": float(sum(r["success_pre"] for r in valid_rows) / denom),
                    "success_post": float(sum(r["success_post"] for r in valid_rows) / denom),
                    "improved_rate": float(sum(r["improved"] for r in valid_rows) / denom),
                }
            )
        else:
            metrics.update(
                {
                    "min_pre": -1.0,
                    "min_post": -1.0,
                    "avg_pre": -1.0,
                    "avg_post": -1.0,
                    "gain_min": 0.0,
                    "gain_avg": 0.0,
                    "success_pre": 0.0,
                    "success_post": 0.0,
                    "improved_rate": 0.0,
                }
            )

        out_csv = os.path.join(self.args.model_dir, f"targeted_eval_ep{episode}.csv")
        os.makedirs(self.args.model_dir, exist_ok=True)
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
            writer.writeheader()
            writer.writerows(rows)
        metrics["csv"] = out_csv

        logger.success(
            "TargetedEval: "
            f"pairs={metrics['pairs_total']} valid={metrics['pairs_valid']} "
            f"min(pre->post)={metrics['min_pre']:.4f}->{metrics['min_post']:.4f} "
            f"gain={metrics['gain_min']:.4f} "
            f"success@{self.success_threshold:.2f}={metrics['success_post']:.2%} "
            f"improved={metrics['improved_rate']:.2%}"
        )
        cleanup_intermediate_files(self.env.save_path, episode_binaries=None)
        return metrics


def train_ppo(args):
    """
    PPO è®­ç»ƒä¸»å‡½æ•°
    """
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    default_log_dir = os.path.join(project_root, 'log')
    default_log_path = os.path.join(default_log_dir, 'train.log')
    train_log_path = args.log_path or default_log_path
    log_dir = os.path.dirname(os.path.abspath(train_log_path))
    os.makedirs(log_dir, exist_ok=True)

    def _console_log_filter(record):
        level = record["level"].name
        if level in ("WARNING", "ERROR", "CRITICAL", "SUCCESS"):
            return True
        if record["name"] not in ["ppo_trainer", 'run_one']:
            return False
        msg = record["message"]
        return (
            msg.startswith("PPO è®­ç»ƒå¯åŠ¨") or
            msg.startswith("æ•°æ®é›†:") or
            msg.startswith("ä¿å­˜è·¯å¾„:") or
            msg.startswith("TensorBoard:") or
            msg.startswith("å›åˆæ€»ç»“:") or
            msg.startswith("action_stats:") or
            msg.startswith("loc_validç»Ÿè®¡:") or
            msg.startswith("TargetedEval:")
        )

    def _file_log_filter(record):
        level = record["level"].name
        if level in ("WARNING", "ERROR", "CRITICAL", "SUCCESS"):
            return True
        name = record["name"]
        msg = record["message"]
        if name == "ppo_trainer":
            return (
                msg.startswith("PPO è®­ç»ƒå¯åŠ¨") or
                msg.startswith("æ•°æ®é›†:") or
                msg.startswith("ä¿å­˜è·¯å¾„:") or
                msg.startswith("TensorBoard:") or
                msg.startswith("å›åˆæ€»ç»“:") or
                msg.startswith("action_stats:") or
                msg.startswith("loc_validç»Ÿè®¡:") or
                msg.startswith("TargetedEval:") or
                "è®­ç»ƒç»“æŸ" in msg
            )
        if name == "ppo_agent" and msg.startswith("âœ… æ¨¡å‹å·²ä¿å­˜"):
            return True
        return False

    logger.remove()
    logger.add(
        sys.stderr,
        level="INFO",
        filter=_console_log_filter,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
    )
    logger.add(
        train_log_path,
        level="INFO",
        filter=_file_log_filter,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
    )

    logger.info("PPO è®­ç»ƒå¯åŠ¨ (Multi-Sample Mode)")
    logger.info(f"æ•°æ®é›†: {args.dataset}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    logger.info(f"æ£€æµ‹æ–¹æ³•: {args.detection_method}")
    
    os.makedirs(args.save_path, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    tensorboard_dir = os.path.join(args.model_dir, 'tensorboard')
    writer = SummaryWriter(log_dir=tensorboard_dir)
    logger.info(f"TensorBoard: {tensorboard_dir}")
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    env = BinaryPerturbationEnv(
        save_path=args.save_path,
        dataset_path=args.dataset,
        sample_hold_interval=args.sample_hold_interval, # Hold-N ç­–ç•¥
        max_steps=args.max_steps,
        detection_method=args.detection_method,
        safe_checkpoint_dir=args.safe_checkpoint_dir,
        safe_i2v_dir=args.safe_i2v_dir,
        safe_use_gpu=args.safe_use_gpu,
        safe_cache_enabled=(args.detection_method == "safe" and not args.no_safe_cache),
        jtrans_model_dir=args.jtrans_model_dir,
        jtrans_tokenizer_dir=args.jtrans_tokenizer_dir,
        jtrans_use_gpu=args.jtrans_use_gpu,
        feature_mode=args.feature_mode,
        seed=args.seed,
        stall_limit=args.stall_limit,
        progress_eps=args.progress_eps,
        progress_reward_eps=args.progress_reward_eps,
        include_schedule_feature=args.include_schedule_feature,
        strict_invalid_loc=(not args.non_strict_invalid_loc),
        hold_min=args.hold_min,
        hold_max=args.hold_max,
    )
    env.set_state_dim(args.state_dim)
    if args.detection_method == "safe":
        safe_target_start = 0.9
        safe_target_end = 0.4
        safe_target_decay_episodes = 1200
        env.target_score = safe_target_start
        env.no_change_penalty = 0.05
        logger.success(
            f"[SAFE train] target_score linear decay {safe_target_start}->{safe_target_end} over {safe_target_decay_episodes} eps; "
            f"no_change_penalty={env.no_change_penalty}"
        )
    elif args.detection_method == "jtrans":
        jtrans_target_start = 0.9
        jtrans_target_end = 0.4
        jtrans_target_decay_episodes = 1200
        env.target_score = jtrans_target_start
        logger.success(
            f"[JTRANS train] target_score linear decay {jtrans_target_start}->{jtrans_target_end} over {jtrans_target_decay_episodes} eps"
        )

    agent = PPOAgent(
        state_dim=args.state_dim,
        n_actions=env.n_actions,
        action_map=list(env.action_ids),
        lr=args.lr,
        gamma=args.gamma,
        epsilon=args.epsilon,
        n_locs=args.n_locs,
        device='cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    )

    targeted_evaluator = None
    if args.targeted_eval_interval > 0:
        try:
            targeted_evaluator = TargetedAttackEvaluator(args)
            logger.success(
                "TargetedEval: "
                f"enabled interval={args.targeted_eval_interval} "
                f"pairs={args.targeted_eval_pairs} "
                f"max_target_variants={args.targeted_eval_max_target_variants} "
                f"eval_steps={args.targeted_eval_max_steps} "
                f"threshold={args.targeted_eval_threshold}"
            )
        except Exception as e:
            logger.warning(f"TargetedEval: åˆå§‹åŒ–å¤±è´¥ï¼Œå·²ç¦ç”¨ ({e})")
            targeted_evaluator = None

    action_ids = list(getattr(env, "action_ids", []))
    action_stats = {aid: {'count': 0, 'reward_sum': 0.0, 'success': 0} for aid in action_ids}
    
    if args.resume and os.path.exists(args.resume):
        agent.load(args.resume)
    
    # log_file = os.path.join(args.model_dir, 'training_log.txt')
    
    episode_binaries = []
    
    # æ»‘åŠ¨çª—å£ç»Ÿè®¡
    success_window = deque(maxlen=50)
    similarity_drop_window = deque(maxlen=50)
    
    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    success_count = 0
    best_score = float('inf')
    info = {}  # åˆå§‹åŒ– infoï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
   
    global_total_steps = 0
    global_loc_total_steps = 0
    global_loc_invalid_steps = 0
    start_episode = 0

    trainer_state = _load_trainer_state(args.resume) if args.resume else None
    if trainer_state:
        start_episode = int(trainer_state.get("episode", -1)) + 1
        global_total_steps = int(trainer_state.get("global_total_steps", 0))
        global_loc_total_steps = int(trainer_state.get("global_loc_total_steps", 0))
        global_loc_invalid_steps = int(trainer_state.get("global_loc_invalid_steps", 0))
        success_count = int(trainer_state.get("success_count", 0))
        best_score = float(trainer_state.get("best_score", float('inf')))
        success_window = deque(trainer_state.get("success_window", []), maxlen=50)
        similarity_drop_window = deque(trainer_state.get("similarity_drop_window", []), maxlen=50)
        logger.info(f"ä»æ–­ç‚¹æ¢å¤è®­ç»ƒ: start_episode={start_episode}")
    if start_episode >= args.episodes:
        logger.warning(f"resume è¿›åº¦å·²è¾¾åˆ° args.episodes ({args.episodes}), æ— éœ€ç»§ç»­è®­ç»ƒ")
        return
    def _make_trainer_state(episode):
        return {
            "episode": int(episode),
            "global_total_steps": int(global_total_steps),
            "global_loc_total_steps": int(global_loc_total_steps),
            "global_loc_invalid_steps": int(global_loc_invalid_steps),
            "success_count": int(success_count),
            "best_score": float(best_score),
            "success_window": list(success_window),
            "similarity_drop_window": list(similarity_drop_window),
        }

    try:
        pbar = tqdm(total=args.episodes - start_episode, desc="Training", unit="ep", dynamic_ncols=True)
        for episode in range(start_episode, args.episodes):
            logger.info("=" * 60)
            logger.info(f"å›åˆ {episode + 1}/{args.episodes}")
            # è®­ç»ƒåŠ¨æ€ï¼šåŸºäºæœ€è¿‘æˆåŠŸç‡è°ƒæ•´ç›®æ ‡åˆ†æ•°ï¼ˆé¿å…å•çº¯çº¿æ€§ä¸‹é™ï¼‰
            recent_success_rate = np.mean(success_window) if success_window else 0.0
            if args.detection_method == "safe":
                progress = min(1.0, episode / max(1, safe_target_decay_episodes))
                linear_target = safe_target_start + (safe_target_end - safe_target_start) * progress
                safe_success_gate = 0.55
                step = abs(safe_target_start - safe_target_end) / max(1, safe_target_decay_episodes)
                if recent_success_rate >= safe_success_gate:
                    env.target_score = max(linear_target, env.target_score - step)
                # å¦åˆ™ä¿æŒå½“å‰ target_scoreï¼Œä¸ç»§ç»­é™ä½
                env.target_score = max(safe_target_end, min(safe_target_start, env.target_score))
                if episode % 10 == 0:
                    logger.success(
                        f"[SAFE train] target_score={env.target_score:.4f} (ep={episode}) "
                        f"sr={recent_success_rate:.2f} gate={safe_success_gate:.2f}"
                    )
            elif args.detection_method == "jtrans":
                progress = min(1.0, episode / max(1, jtrans_target_decay_episodes))
                linear_target = jtrans_target_start + (jtrans_target_end - jtrans_target_start) * progress
                jtrans_success_gate = 0.55
                step = abs(jtrans_target_start - jtrans_target_end) / max(1, jtrans_target_decay_episodes)
                if recent_success_rate >= jtrans_success_gate:
                    env.target_score = max(linear_target, env.target_score - step)
                env.target_score = max(jtrans_target_end, min(jtrans_target_start, env.target_score))
                if episode % 10 == 0:
                    logger.success(
                        f"[JTRANS train] target_score={env.target_score:.4f} (ep={episode}) "
                        f"sr={recent_success_rate:.2f} gate={jtrans_success_gate:.2f}"
                    )
            
            state = env.reset()
            
            episode_actions = [] 
            initial_score = 1.0 # ã€ä¼˜åŒ–ã€‘é»˜è®¤åˆå§‹ä¸º1.0ï¼Œé˜²æ­¢ç¬¬ä¸€æ­¥æ²¡å–åˆ°scoreå¯¼è‡´è®¡ç®—é”™è¯¯
            
            episode_reward = 0
            episode_loc_total_steps = 0
            episode_loc_invalid_steps = 0
            last_binary_info = None
            should_skip_update = False
            episode_done = False  # æ ‡è®° episode æ˜¯å¦æ­£å¸¸ç»“æŸ
            
            for step in range(args.max_steps):
                global_total_steps += 1 

                loc_mask = env.get_loc_mask(args.n_locs)
                joint_idx, loc_idx, act_idx, actual_action, log_prob, value = agent.select_action(
                    state, explore=True, loc_mask=loc_mask
                )
                episode_actions.append(actual_action)
                prev_state = state
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(actual_action, loc_idx)
                logger.success(f"Step {step}: Loc {loc_idx}, Action {actual_action} (JointIdx {joint_idx}), reward {reward:.4f}")
                # input("step down, press enter to continue")
                episode_reward += reward
                state = next_state

                loc_valid = info.get('loc_valid')
                if loc_valid is not None:
                    episode_loc_total_steps += 1
                    global_loc_total_steps += 1
                    if not loc_valid:
                        episode_loc_invalid_steps += 1
                        global_loc_invalid_steps += 1
                
                
                if step % 100 == 0:
                    # è®°å½•æ¯æ­¥æŒ‡æ ‡ (å½“å‰è®¾ç½®ï¼šæ¯æ­¥éƒ½è®°ï¼Œå¦‚æœå¤ªæ…¢å¯æ”¹ä¸º if step % 5 == 0)
                    writer.add_scalar('Step/Shaped_Reward', reward, global_total_steps)            # Agent æ¯åšä¸€æ­¥åŠ¨ä½œå¾—åˆ°çš„å³æ—¶åé¦ˆï¼ˆåŒ…å«è¿›æ­¥åˆ†ã€æƒ©ç½šåˆ†ç­‰ï¼‰ã€‚
                    writer.add_scalar('Step/Critic_Value', value, global_total_steps)                     # Critic ç½‘ç»œï¼ˆè£åˆ¤ï¼‰è®¤ä¸ºâ€œå½“å‰è¿™ä¸ªçŠ¶æ€ï¼Œæœªæ¥èƒ½æ‹¿å¤šå°‘åˆ†â€ã€‚
                    if 'score' in info:
                        writer.add_scalar('Step/Similarity_Score', info['score'], global_total_steps)     # æ¯ä¸€æ­¥å˜å¼‚åçš„ä»£ç ä¸åŸä»£ç çš„ç›¸ä¼¼åº¦ã€‚

                # ç»Ÿè®¡åŠ¨ä½œçº§åˆ«çš„ reward/success
                stat = action_stats.setdefault(actual_action, {'count': 0, 'reward_sum': 0.0, 'success': 0})
                stat['count'] += 1
                stat['reward_sum'] += reward
                if info.get('score', 1.0) < env.target_score:
                    stat['success'] += 1

                # å­˜å‚¨ç»éªŒ
                agent.store_transition(prev_state, joint_idx, reward, log_prob, value, done, loc_mask=loc_mask)
 
                if 'binary' in info:
                    last_binary_info = {
                        'episode': episode, 'step': step,
                        'binary': info['binary'], 'score': info.get('score', 1.0),
                        'func': info.get('target_func', 'unknown') # å­˜ä¸€ä¸‹å‡½æ•°å
                    }
                
                # âœ… æˆåŠŸæ£€æŸ¥ä¸é”™è¯¯å¤„ç†
                if done:
                    if info.get('should_reset', False):
                        logger.warning("âš ï¸ é”™è¯¯å‘ç”Ÿï¼Œå¼ºåˆ¶åˆ‡æ¢ç›®æ ‡")
                        should_skip_update = True
                        state = env.reset(force_switch=True)
                    
                    episode_done = True
                    break
            
            # âœ… ç»Ÿä¸€çš„ç»Ÿè®¡é€»è¾‘
            # 1. ç»Ÿè®¡æˆåŠŸç‡å’Œé™åˆ† (ä½¿ç”¨ last_binary_info æ›´å®‰å…¨)
            final_score = last_binary_info['score'] if last_binary_info else 1.0
            target_func = last_binary_info['func'] if last_binary_info else "unknown"

            is_success = final_score < env.target_score
            success_window.append(1 if is_success else 0)
            similarity_drop_window.append(max(0.0, initial_score - final_score))

            if is_success:
                success_count += 1
                logger.success(f"ğŸ‰ æ”»ç ´! ç›®æ ‡: {info.get('target_func')} | åˆ†æ•°: {final_score:.4f}")
                with open(os.path.join(args.save_path, 'success.log'), 'a') as f:
                    f.write(f"Ep {episode}, Func: {info.get('target_func')}, Score: {final_score:.4f}\n")

            if last_binary_info:
                episode_binaries.append(last_binary_info)

            # 2. å¦‚æœå‡ºé”™è·³è¿‡æ›´æ–°
            if should_skip_update:
                agent.clear_memory()
                pbar.update(1)
                continue
            
            # æˆªæ–­å›åˆæ—¶åš bootstrap
            next_value = 0.0
            if not episode_done:
                next_value = agent.estimate_value(state)
            
            # PPO æ›´æ–°
            update_info = agent.update(next_value=next_value)
            loss = update_info['loss']

            # æ‰“å°åŠ¨ä½œåˆ†å¸ƒ
            agent.log_action_distribution(episode)
            
            # === Episode çº§åˆ«è®°å½• (æ ¸å¿ƒ) ===
            current_success_rate = np.mean(success_window) if success_window else 0.0
            avg_drop = np.mean(similarity_drop_window) if similarity_drop_window else 0.0
            episode_loc_invalid_ratio = (
                episode_loc_invalid_steps / episode_loc_total_steps
                if episode_loc_total_steps > 0 else 0.0
            )
            global_loc_invalid_ratio = (
                global_loc_invalid_steps / global_loc_total_steps
                if global_loc_total_steps > 0 else 0.0
            )

            logger.success(
                f"å›åˆæ€»ç»“: æ€»å¥–={episode_reward:.2f} | æ»‘åŠ¨æˆåŠŸç‡={current_success_rate:.2f} | å¹³å‡é™åˆ†={avg_drop:.2f} "
                f"| loc_valid=False(æœ¬å›åˆ)={episode_loc_invalid_ratio:.2%} | loc_valid=False(å…¨å±€)={global_loc_invalid_ratio:.2%} "
                f"| æ­¥æ•°={step+1} | ç›®æ ‡å‡½æ•°={target_func} | ç›®æ ‡äºŒè¿›åˆ¶={last_binary_info['binary'] if last_binary_info else 'N/A'}"
            )
            logger.info(
                f"loc_validç»Ÿè®¡: æœ¬å›åˆæ— æ•ˆæ¯”ä¾‹={episode_loc_invalid_ratio:.2%} | å…¨å±€æ— æ•ˆæ¯”ä¾‹={global_loc_invalid_ratio:.2%} "
                f"| æœ¬å›åˆç»Ÿè®¡æ­¥æ•°={episode_loc_total_steps} | å…¨å±€ç»Ÿè®¡æ­¥æ•°={global_loc_total_steps}"
            )
            if episode % 50 == 0 and action_stats:
                parts = []
                for aid in sorted(action_stats.keys()):
                    stat = action_stats[aid]
                    if stat['count'] == 0:
                        continue
                    avg_r = stat['reward_sum'] / stat['count']
                    succ = stat['success'] / stat['count']
                    parts.append(f"a{aid}:cnt={stat['count']} avgR={avg_r:.3f} succ={succ:.1%}")
                if parts:
                    logger.info("action_stats: " + " | ".join(parts))
                for stat in action_stats.values():
                    stat['count'] = 0
                    stat['reward_sum'] = 0.0
                    stat['success'] = 0
            
            writer.add_scalar('Main/Success_Rate_MA50', current_success_rate, episode)      # æœ€è¿‘ 50 ä¸ªå›åˆä¸­ï¼ŒæˆåŠŸç»•è¿‡æ£€æµ‹ï¼ˆåˆ†æ•° < 0.4ï¼‰çš„æ¯”ä¾‹ã€‚
            writer.add_scalar('Main/Similarity_Drop_MA50', avg_drop, episode)               # æœ€è¿‘ 50 ä¸ªå›åˆä¸­ï¼Œå¹³å‡æŠŠç›¸ä¼¼åº¦é™ä½äº†å¤šå°‘ï¼ˆåˆå§‹åˆ† 1.0 - æœ€ç»ˆåˆ†ï¼‰
            writer.add_scalar('Main/Episode_Reward', episode_reward, episode)               # Agent åœ¨ä¸€ä¸ªå›åˆå†…æ‹¿åˆ°çš„æ‰€æœ‰å¥–åŠ±ä¹‹å’Œã€‚
            writer.add_scalar('Main/Episode_Length', step + 1, episode)                      # ä¸€ä¸ªå›åˆå†…æ€»å…±æ‰§è¡Œäº†å¤šå°‘æ­¥ã€‚
            writer.add_scalar('Debug/Loc_Invalid_Ratio_Episode', episode_loc_invalid_ratio, episode)
            writer.add_scalar('Debug/Loc_Invalid_Ratio_Global', global_loc_invalid_ratio, episode)
            writer.add_histogram('Debug/Action_Distribution', np.array(episode_actions), episode)   # åœ¨å½“å‰å›åˆä¸­ï¼ŒAgent é€‰æ‹©äº†å“ªäº›å˜å¼‚åŠ¨ä½œï¼ˆAction 0-5ï¼‰ã€‚
            writer.add_scalar('Debug/Policy_Loss', loss, episode)                           # PPO ç®—æ³•æ›´æ–°æ—¶çš„ Loss å€¼ã€‚
            writer.add_scalar('Debug/Advantage_Mean_Raw', update_info['adv_mean_raw'], episode)
            writer.add_scalar('Debug/Advantage_Std_Raw', update_info['adv_std_raw'], episode)
            writer.add_scalar('Debug/Advantage_AbsMean_Raw', update_info['adv_abs_mean_raw'], episode)
            writer.add_scalar('Debug/Advantage_MaxAbs_Raw', update_info['adv_max_abs_raw'], episode)

            if targeted_evaluator is not None and (episode + 1) % args.targeted_eval_interval == 0:
                try:
                    agent.policy.eval()
                    t_metrics = targeted_evaluator.evaluate(agent, episode + 1)
                finally:
                    agent.policy.train()
                if t_metrics is not None:
                    writer.add_scalar('Targeted/Pairs_Valid', t_metrics['pairs_valid'], episode)
                    writer.add_scalar('Targeted/MinSim_Pre', t_metrics['min_pre'], episode)
                    writer.add_scalar('Targeted/MinSim_Post', t_metrics['min_post'], episode)
                    writer.add_scalar('Targeted/MinSim_Gain', t_metrics['gain_min'], episode)
                    writer.add_scalar('Targeted/AvgSim_Pre', t_metrics['avg_pre'], episode)
                    writer.add_scalar('Targeted/AvgSim_Post', t_metrics['avg_post'], episode)
                    writer.add_scalar('Targeted/AvgSim_Gain', t_metrics['gain_avg'], episode)
                    writer.add_scalar('Targeted/Success_Pre', t_metrics['success_pre'], episode)
                    writer.add_scalar('Targeted/Success_Post', t_metrics['success_post'], episode)
                    writer.add_scalar('Targeted/Improved_Rate', t_metrics['improved_rate'], episode)

            pbar.set_postfix_str(
                f"sr={current_success_rate:.2f} drop={avg_drop:.2f} "
                f"loss={loss:.2f} adv_std={update_info['adv_std_raw']:.2f}"
            )
            pbar.update(1)
            
            # å†™æ—¥å¿—æ–‡ä»¶
            # with open(log_file, 'a') as f:
            #     f.write(
            #         f"{episode},{step+1},{episode_reward:.4f},{loss:.4f},"
            #         f"{current_success_rate:.2f},{update_info['adv_std_raw']:.4f}\n"
            #     )
            
            # ä¿å­˜æ¨¡å‹
            if (episode + 1) % args.save_interval == 0:
                agent.save(
                    os.path.join(args.model_dir, f'ppo_model_ep{episode+1}.pt'),
                    extra_state=_make_trainer_state(episode),
                )
            
            if 'score' in info and info['score'] < best_score:
                best_score = info['score']
                agent.save(
                    os.path.join(args.model_dir, 'ppo_model_best.pt'),
                    extra_state=_make_trainer_state(episode),
                )

            # å®šæœŸæ¸…ç†
            if episode % 40 == 0:   
                cleanup_intermediate_files(args.save_path, episode_binaries)
    
    except KeyboardInterrupt:
        logger.warning("è®­ç»ƒä¸­æ–­")
    
    finally:
        if 'pbar' in locals():
            pbar.close()
        last_episode = locals().get("episode", start_episode - 1)
        agent.save(
            os.path.join(args.model_dir, 'ppo_model_final.pt'),
            extra_state=_make_trainer_state(last_episode),
        )
        writer.close()
        
        # ã€ä¼˜åŒ–ã€‘å–æ¶ˆæ³¨é‡Šï¼Œç¡®ä¿é€€å‡ºæ—¶æ¸…ç†åƒåœ¾
        cleanup_intermediate_files(args.save_path, episode_binaries)
        
        # ä¿å­˜æ¸…å•
        manifest_path = os.path.join(args.model_dir, 'episode_binaries.txt')
        with open(manifest_path, 'w') as f:
            for item in episode_binaries:
                f.write(f"{item['episode']},{item['step']},{item['binary']},{item['score']:.4f}\n")
        logger.info(f"âœ“ è®­ç»ƒç»“æŸï¼Œæ•°æ®å·²ä¿å­˜")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', required=True)
    parser.add_argument('--save-path', required=True)
    parser.add_argument('--state-dim', type=int, default=256) # é»˜è®¤256ç»´
    parser.add_argument('--lr', type=float, default=5e-5)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--epsilon', type=float, default=0.15)
    parser.add_argument('--n-locs', type=int, default=3)
    parser.add_argument('--episodes', type=int, default=6000)
    parser.add_argument('--max-steps', type=int, default=40)
    parser.add_argument('--save-interval', type=int, default=50)
    parser.add_argument('--sample-hold-interval', type=int, default=15)
    parser.add_argument('--stall-limit', type=int, default=8)
    parser.add_argument('--progress-eps', type=float, default=5e-4)
    parser.add_argument('--progress-reward-eps', type=float, default=2e-3)
    parser.add_argument('--include-schedule-feature', action='store_true')
    parser.add_argument('--non-strict-invalid-loc', action='store_true')
    parser.add_argument('--hold-min', type=int, default=4)
    parser.add_argument('--hold-max', type=int, default=10)
    parser.add_argument('--model-dir', default='./rl_models')
    parser.add_argument('--resume', default=None)
    parser.add_argument('--use-gpu', action='store_true')
    parser.add_argument('--log-path', default=None, help='è®­ç»ƒæ—¥å¿—è·¯å¾„ï¼ˆé»˜è®¤ log/train.logï¼‰')
    parser.add_argument('--detection-method', choices=['asm2vec', 'safe', 'jtrans'], default='asm2vec')
    # SAFE ç›¸å…³å‚æ•°
    parser.add_argument('--safe-checkpoint-dir', default=None)
    parser.add_argument('--safe-i2v-dir', default=None)
    parser.add_argument('--safe-use-gpu', action='store_true')
    parser.add_argument('--no-safe-cache', action='store_true', help='Disable SAFE cache reuse during training')
    # jtrans
    parser.add_argument('--jtrans-model-dir', default=None)
    parser.add_argument('--jtrans-tokenizer-dir', default=None)
    parser.add_argument('--jtrans-use-gpu', action='store_true')
    parser.add_argument(
        '--feature-mode',
        choices=['full', 'no_progress', 'no_api', 'no_progress_api', 'no_section_c'],
        default='full'
    )
    parser.add_argument('--seed', type=int, default=None)
    # å®šå‘æ”»å‡»è¯„æµ‹ï¼ˆTargeted Attack Evalï¼‰
    parser.add_argument('--targeted-eval-interval', type=int, default=0,
                        help='æ¯éš”å¤šå°‘ä¸ª episode æ‰§è¡Œä¸€æ¬¡å®šå‘æ”»å‡»è¯„æµ‹ï¼ˆ0 è¡¨ç¤ºå…³é—­ï¼‰')
    parser.add_argument('--targeted-eval-dataset', default=None,
                        help='å®šå‘è¯„æµ‹æ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤å¤ç”¨ --datasetï¼‰')
    parser.add_argument('--targeted-eval-pairs', type=int, default=8,
                        help='æ¯æ¬¡è¯„æµ‹éšæœº attacker-target é…å¯¹æ•°')
    parser.add_argument('--targeted-eval-max-target-variants', type=int, default=4,
                        help='æ¯ä¸ª target identity æœ€å¤šä½¿ç”¨å¤šå°‘ä¸ªç¼–è¯‘å˜ä½“')
    parser.add_argument('--targeted-eval-min-target-variants', type=int, default=2,
                        help='æ¯ä¸ª target identity è‡³å°‘éœ€è¦å¤šå°‘ä¸ªå¯ç”¨å˜ä½“')
    parser.add_argument('--targeted-eval-max-steps', type=int, default=30,
                        help='å®šå‘è¯„æµ‹æ¯ä¸ª pair çš„æ”»å‡»æ­¥æ•°é¢„ç®—')
    parser.add_argument('--targeted-eval-threshold', type=float, default=0.85,
                        help='å®šå‘æˆåŠŸé˜ˆå€¼ï¼šmin(sim_to_target_variants) >= threshold')
    parser.add_argument('--targeted-eval-seed', type=int, default=1234,
                        help='å®šå‘è¯„æµ‹éšæœºç§å­')
    parser.add_argument('--targeted-eval-explore', action='store_true',
                        help='å®šå‘è¯„æµ‹æ—¶æ˜¯å¦ä½¿ç”¨éšæœºé‡‡æ ·åŠ¨ä½œï¼ˆé»˜è®¤è´ªå¿ƒï¼‰')
    
    args = parser.parse_args()

    if args.detection_method == "jtrans":
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        jtrans_root = os.path.join(project_root, "detection_model", "jTrans")
        if args.jtrans_model_dir is None:
            args.jtrans_model_dir = os.path.join(jtrans_root, "models", "jTrans-finetune")
        if args.jtrans_tokenizer_dir is None:
            args.jtrans_tokenizer_dir = os.path.join(jtrans_root, "jtrans_tokenizer")
    
    # æ¸…ç†æ—§æ—¥å¿—
    log_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'log/uroboro.log')
    if os.path.exists(log_path): os.remove(log_path)
        
    train_ppo(args)
