#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Trainer for Binary Code Perturbation
PPO è®­ç»ƒå™¨ï¼ˆç›´æ¥è°ƒç”¨ç¯å¢ƒï¼‰
"""
import os
import numpy as np
import torch
from ppo_agent import PPOAgent, RewardShaper
import argparse
from loguru import logger
import sys
import shutil
import glob
from torch.utils.tensorboard import SummaryWriter

# å¯¼å…¥ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env_wrapper import BinaryPerturbationEnv


def cleanup_intermediate_files(save_path, episode_binaries=None):
    """
    æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ–‡ä»¶
    
    æ¸…ç†ç­–ç•¥:
    1. save_path ä¸­:
       - ä¿ç•™ success.log
       - ä¿ç•™æ¯ä¸ªå›åˆæœ€åä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶æ‰€åœ¨çš„ *_container ç›®å½•
       - åˆ é™¤æ‰€æœ‰ tmp_* ä¸´æ—¶ç›®å½•
       - åˆ é™¤å…¶ä»–ä¸´æ—¶æ–‡ä»¶
    2. rl_output ä¸­:
       - åˆ é™¤æ‰€æœ‰ mutant_*.bin* æ–‡ä»¶ï¼ˆå…¨éƒ¨æ¸…ç©ºï¼‰
    
    å‚æ•°:
        save_path: ä¿å­˜è·¯å¾„ï¼ˆfunction_container_* ç›®å½•ï¼‰
        episode_binaries: æ¯ä¸ªå›åˆçš„æœ€åä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶åˆ—è¡¨ï¼ˆéœ€è¦ä¿ç•™ï¼‰
    """
    if not os.path.exists(save_path):
        return
    
    logger.info(f"å¼€å§‹æ¸…ç†ä¸­é—´æ–‡ä»¶: {save_path}")
    success_log_path = os.path.join(save_path, 'success.log')
    
    # æå–éœ€è¦ä¿ç•™çš„äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
    binaries_to_keep = set()
    if episode_binaries:
        for item in episode_binaries:
            if 'binary' in item and item['binary']:
                binaries_to_keep.add(os.path.abspath(item['binary']))
        logger.info(f"å°†ä¿ç•™ {len(binaries_to_keep)} ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶")
    
    deleted_count = 0
    deleted_size = 0
    
    # æ¸…ç† tmp_* ä¸´æ—¶ç›®å½•
    tmp_pattern = os.path.join(save_path, 'tmp_*')
    for tmp_dir in glob.glob(tmp_pattern):
        if os.path.isdir(tmp_dir):
            try:
                # è®¡ç®—å¤§å°
                size = sum(
                    os.path.getsize(os.path.join(dirpath, filename))
                    for dirpath, dirnames, filenames in os.walk(tmp_dir)
                    for filename in filenames
                )
                shutil.rmtree(tmp_dir)
                deleted_count += 1
                deleted_size += size
                # logger.debug(f"  åˆ é™¤ä¸´æ—¶ç›®å½•: {os.path.basename(tmp_dir)}")
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½• {tmp_dir}: {e}")
    
    # æ¸…ç† *_container å®¹å™¨ç›®å½•ï¼ˆä½†ä¿ç•™æ¯ä¸ªå›åˆçš„æœ€åä¸€ä¸ªï¼‰
    container_pattern = os.path.join(save_path, '*_container')
    for container_dir in glob.glob(container_pattern):
        if os.path.isdir(container_dir):
            # æ£€æŸ¥è¿™ä¸ªå®¹å™¨ç›®å½•æ˜¯å¦åŒ…å«éœ€è¦ä¿ç•™çš„äºŒè¿›åˆ¶æ–‡ä»¶
            # å°†å®¹å™¨ç›®å½•è½¬æ¢ä¸ºç»å¯¹è·¯å¾„è¿›è¡Œæ¯”è¾ƒ
            container_dir_abs = os.path.abspath(container_dir)
            should_keep = False
            
            if binaries_to_keep:
                for binary_path in binaries_to_keep:
                    # ä¸¤è¾¹éƒ½æ˜¯ç»å¯¹è·¯å¾„ï¼Œæ£€æŸ¥äºŒè¿›åˆ¶æ–‡ä»¶æ˜¯å¦åœ¨è¿™ä¸ªå®¹å™¨ç›®å½•ä¸‹
                    if binary_path.startswith(container_dir_abs + os.sep) or binary_path == container_dir_abs:
                        # logger.debug(f"  ä¿ç•™å®¹å™¨ç›®å½•ï¼ˆåŒ…å«å›åˆæœ€ç»ˆæ–‡ä»¶ï¼‰: {os.path.basename(container_dir)}")
                        # logger.debug(f"    -> åŒ…å«æ–‡ä»¶: {os.path.basename(binary_path)}")
                        should_keep = True
                        break
            
            if should_keep:
                continue
            
            try:
                # è®¡ç®—å¤§å°
                size = sum(
                    os.path.getsize(os.path.join(dirpath, filename))
                    for dirpath, dirnames, filenames in os.walk(container_dir)
                    for filename in filenames
                )
                shutil.rmtree(container_dir)
                deleted_count += 1
                deleted_size += size
                logger.debug(f"  åˆ é™¤å®¹å™¨ç›®å½•: {os.path.basename(container_dir)}")
            except Exception as e:
                logger.warning(f"  æ— æ³•åˆ é™¤å®¹å™¨ç›®å½• {container_dir}: {e}")
    
    # æ¸…ç†å…¶ä»–ä¸´æ—¶æ–‡ä»¶ï¼ˆé™¤äº† success.logï¼‰
    for item in os.listdir(save_path):
        item_path = os.path.join(save_path, item)
        
        # è·³è¿‡ success.log
        if item == 'success.log':
            continue
        
        # è·³è¿‡ç›®å½•ï¼ˆå·²å¤„ç†ï¼‰
        if os.path.isdir(item_path):
            continue
        
        # åˆ é™¤å…¶ä»–æ–‡ä»¶
        try:
            size = os.path.getsize(item_path)
            os.remove(item_path)
            deleted_count += 1
            deleted_size += size
            logger.debug(f"  åˆ é™¤æ–‡ä»¶: {item}")
        except Exception as e:
            logger.warning(f"  æ— æ³•åˆ é™¤æ–‡ä»¶ {item}: {e}")
    
    # æ¸…ç† rl_output ä¸­çš„æ‰€æœ‰ä¸­é—´ mutant æ–‡ä»¶ï¼ˆå…¨éƒ¨åˆ é™¤ï¼‰
    # rl_output è·¯å¾„ï¼šä»å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•æ¨å¯¼
    current_dir = os.path.dirname(os.path.abspath(__file__))
    rl_output_dir = os.path.join(current_dir, 'rl_output')
    
    if os.path.exists(rl_output_dir):
        mutant_pattern = os.path.join(rl_output_dir, 'mutant_*.bin*')
        mutant_files = glob.glob(mutant_pattern)
        
        if mutant_files:
            logger.info(f"æ¸…ç† rl_output ä¸­é—´æ–‡ä»¶: {rl_output_dir} ({len(mutant_files)} ä¸ªæ–‡ä»¶)")
            for file_path in mutant_files:
                try:
                    size = os.path.getsize(file_path)
                    os.remove(file_path)
                    deleted_count += 1
                    deleted_size += size
                    # logger.debug(f"  åˆ é™¤ rl_output æ–‡ä»¶: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"  æ— æ³•åˆ é™¤ rl_output æ–‡ä»¶ {file_path}: {e}")
            logger.info(f"âœ“ å·²åˆ é™¤ rl_output ä¸­çš„æ‰€æœ‰ mutant æ–‡ä»¶ ({len(mutant_files)} ä¸ª)")
        else:
            logger.debug(f"  rl_output ä¸­æ²¡æœ‰ mutant æ–‡ä»¶")
    
    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
    if deleted_size < 1024:
        size_str = f"{deleted_size} B"
    elif deleted_size < 1024 * 1024:
        size_str = f"{deleted_size / 1024:.2f} KB"
    else:
        size_str = f"{deleted_size / (1024 * 1024):.2f} MB"
    
    logger.success(f"âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªé¡¹ç›®ï¼Œé‡Šæ”¾ {size_str} ç©ºé—´")
    logger.info(f"âœ“ å·²ä¿ç•™: success.log")
    if binaries_to_keep:
        logger.info(f"âœ“ å·²ä¿ç•™: {len(binaries_to_keep)} ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶")


def train_ppo(args):
    """
    PPO è®­ç»ƒä¸»å‡½æ•°
    """
    logger.info("PPO è®­ç»ƒå¯åŠ¨")
    logger.info(f"åŸå§‹äºŒè¿›åˆ¶: {args.binary}")
    logger.info(f"ç›®æ ‡å‡½æ•°: {args.function}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    logger.info(f"æœ€å¤§å›åˆæ•°: {args.episodes}")
    logger.info(f"æœ€å¤§æ­¥æ•°/å›åˆ: {args.max_steps}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_path, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    # åˆå§‹åŒ– TensorBoard
    tensorboard_dir = os.path.join(args.model_dir, 'tensorboard')
    os.makedirs(tensorboard_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=tensorboard_dir)
    logger.info(f"TensorBoard æ—¥å¿—ç›®å½•: {tensorboard_dir}")
    logger.info(f"å¯åŠ¨ TensorBoard: tensorboard --logdir={tensorboard_dir}")
    
    # åˆå§‹åŒ–ç¯å¢ƒï¼ˆç›´æ¥åˆ›å»ºï¼Œæ— éœ€è¿›ç¨‹é€šä¿¡ï¼‰
    logger.info("åˆå§‹åŒ–å˜å¼‚ç¯å¢ƒ...")
    env = BinaryPerturbationEnv(
        original_binary=args.binary,
        function_name=args.function,
        save_path=args.save_path
    )
    # è®¾ç½®çŠ¶æ€ç»´åº¦ï¼Œä¸ç¯å¢ƒä¿æŒä¸€è‡´
    env.set_state_dim(args.state_dim)
    logger.info("ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ âœ“")
    
    # åˆå§‹åŒ– PPO Agentï¼ˆä½¿ç”¨æ”¹è¿›çš„ç½‘ç»œç»“æ„ï¼‰
    agent = PPOAgent(
        state_dim=args.state_dim,
        n_actions=6,
        lr=args.lr,  # é»˜è®¤å·²æ”¹ä¸º 1e-4
        gamma=args.gamma,
        epsilon=args.epsilon,
        device='cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    )
    
    logger.info(f"ç½‘ç»œç»“æ„: Actor-Critic åˆ†ç¦»æ¶æ„")
    logger.info(f"Actor å­¦ä¹ ç‡: {args.lr:.2e}, Critic å­¦ä¹ ç‡: {args.lr * 2:.2e}")
    
    # å¦‚æœå­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åŠ è½½
    if args.resume and os.path.exists(args.resume):
        agent.load(args.resume)
    
    # å¥–åŠ±å¡‘å½¢å™¨
    reward_shaper = RewardShaper(target_score=0.40)
    
    # è®­ç»ƒæ—¥å¿—
    log_file = os.path.join(args.model_dir, 'training_log.txt')
    if os.path.exists(log_file):
        os.remove(log_file) # åˆ é™¤æ—§çš„æ—¥å¿—æ–‡ä»¶
    best_score = float('inf')
    success_count = 0
    
    # ä¿å­˜æ¯ä¸ªå›åˆçš„æœ€åä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
    episode_binaries = []  # å­˜å‚¨æ¯ä¸ªå›åˆçš„æœ€åä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶
    
    try:
        for episode in range(args.episodes):
            logger.info("=" * 80)
            logger.info(f"å›åˆ {episode + 1}/{args.episodes}")
            logger.info("=" * 80)
            
            # é‡ç½®ç¯å¢ƒ
            state = env.reset()
            reward_shaper.reset()
            
            episode_reward = 0
            episode_loss = 0
            global_step = episode * args.max_steps  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨
            last_binary_info = None  # è¿½è¸ªæœ¬å›åˆæœ€åä¸€ä¸ªäºŒè¿›åˆ¶
            
            for step in range(args.max_steps):
                # input(f"In step {step + 1}, Press Enter to continue...")
                # é€‰æ‹©åŠ¨ä½œ
                action_idx, actual_action, log_prob, value = agent.select_action(state, explore=True)
                
                logger.debug(f"æ­¥éª¤ {step + 1}: åŠ¨ä½œ={actual_action} (ç´¢å¼•={action_idx})")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(actual_action)
                
                # å¥–åŠ±å¡‘å½¢
                if 'score' in info:
                    shaped_reward = reward_shaper.compute_reward(
                        info['score'], 
                        info.get('grad', 0), 
                        done, 
                        step
                    )
                else:
                    shaped_reward = reward  # å¦‚æœæ²¡æœ‰ scoreï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¥–åŠ±
                
                if 'score' in info:
                    logger.info(f"  å¥–åŠ±: {reward:.4f} â†’ {shaped_reward:.4f} | åˆ†æ•°: {info['score']:.4f} | æ¢¯åº¦: {info.get('grad', 0):.4f}")
                else:
                    logger.info(f"  å¥–åŠ±: {reward:.4f}")
                
                # è®°å½•æ¯æ­¥æŒ‡æ ‡åˆ° TensorBoard
                current_step = global_step + step
                writer.add_scalar('Step/Raw_Reward', reward, current_step)
                writer.add_scalar('Step/Shaped_Reward', shaped_reward, current_step)
                writer.add_scalar('Step/Value', value, current_step)
                writer.add_scalar('Step/Action', actual_action, current_step)
                if 'score' in info:
                    writer.add_scalar('Step/Similarity_Score', info['score'], current_step)
                if 'grad' in info:
                    writer.add_scalar('Step/Gradient', info['grad'], current_step)
                
                # å­˜å‚¨ç»éªŒ
                agent.store_transition(state, action_idx, shaped_reward, log_prob, value)
                
                episode_reward += shaped_reward
                state = next_state
                
                # æ›´æ–°æœ¬å›åˆæœ€åä¸€ä¸ªäºŒè¿›åˆ¶ä¿¡æ¯
                if 'binary' in info:
                    last_binary_info = {
                        'episode': episode,
                        'step': step,
                        'binary': info['binary'],
                        'score': info.get('score', 1.0)
                    }
                
                # æ£€æŸ¥æˆåŠŸ
                if done:
                    if 'score' in info and info['score'] < 0.40:
                        success_count += 1
                        logger.success(f"ğŸ‰ æˆåŠŸç»•è¿‡æ£€æµ‹! åˆ†æ•°: {info['score']:.4f}")
                        
                        # ä¿å­˜æˆåŠŸæ ·æœ¬ä¿¡æ¯
                        success_log = os.path.join(args.save_path, 'success.log')
                        with open(success_log, 'a') as f:
                            f.write(f"Episode {episode}, Step {step}, Score: {info['score']:.4f}\n")
                            f.write(f"Binary: {info.get('binary', 'unknown')}\n\n")
                    
                    logger.info(f"å›åˆç»“æŸ (æ­¥æ•°: {step + 1})")
                    break
            
            # ä¿å­˜æœ¬å›åˆçš„æœ€åä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
            if last_binary_info is not None:
                episode_binaries.append(last_binary_info)
                logger.debug(f"âœ“ è®°å½•å›åˆ {episode} çš„æœ€ç»ˆäºŒè¿›åˆ¶: {os.path.basename(last_binary_info['binary'])} (åˆ†æ•°: {last_binary_info['score']:.4f})")
            
            # PPO æ›´æ–°
            loss = agent.update()
            episode_loss = loss
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            avg_reward = episode_reward / (step + 1)
            
            logger.info(f"å›åˆæ€»ç»“: æ€»å¥–åŠ±={episode_reward:.4f} | å¹³å‡å¥–åŠ±={avg_reward:.4f} | ç­–ç•¥æŸå¤±={loss:.4f} | æˆåŠŸæ¬¡æ•°={success_count}")
            
            # è®°å½•å›åˆçº§åˆ«æŒ‡æ ‡åˆ° TensorBoard
            writer.add_scalar('Episode/Total_Reward', episode_reward, episode)
            writer.add_scalar('Episode/Average_Reward', avg_reward, episode)
            writer.add_scalar('Episode/Policy_Loss', loss, episode)
            writer.add_scalar('Episode/Steps', step + 1, episode)
            writer.add_scalar('Episode/Success_Count', success_count, episode)
            if 'score' in info:
                writer.add_scalar('Episode/Final_Score', info['score'], episode)
                writer.add_scalar('Episode/Best_Score', best_score, episode)
            
            # ä¿å­˜åˆ°æ—¥å¿—
            with open(log_file, 'a') as f:
                f.write(f"{episode},{step+1},{episode_reward:.4f},{avg_reward:.4f},{loss:.4f}\n")
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if (episode + 1) % args.save_interval == 0:
                model_path = os.path.join(args.model_dir, f'ppo_model_ep{episode+1}.pt')
                agent.save(model_path)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if 'score' in info and info['score'] < best_score:
                best_score = info['score']
                best_model_path = os.path.join(args.model_dir, 'ppo_model_best.pt')
                agent.save(best_model_path)
                logger.success(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (åˆ†æ•°: {best_score:.4f})")
    
    except KeyboardInterrupt:
        logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(args.model_dir, 'ppo_model_final.pt')
        agent.save(final_model_path)
        
        # å…³é—­ TensorBoard writer
        writer.close()
        
        logger.info("=" * 80)
        logger.success("è®­ç»ƒå®Œæˆ")
        logger.info(f"æˆåŠŸç»•è¿‡æ¬¡æ•°: {success_count} | æœ€ä½³åˆ†æ•°: {best_score:.4f}")
        logger.info(f"TensorBoard æ—¥å¿—: {tensorboard_dir}")
        logger.info("=" * 80)
        
        # æ¸…ç†ä¸­é—´æ–‡ä»¶ï¼ˆä¿ç•™ success.log å’Œæ¯ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶ï¼‰
        logger.info("")
        cleanup_intermediate_files(args.save_path, episode_binaries)
        
        # ä¿å­˜å›åˆäºŒè¿›åˆ¶æ–‡ä»¶æ¸…å•
        manifest_path = os.path.join(args.model_dir, 'episode_binaries.txt')
        with open(manifest_path, 'w') as f:
            f.write("# æ¯ä¸ªå›åˆçš„æœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆæ¯ä¸ªå›åˆçš„æœ€åä¸€æ­¥ç”Ÿæˆçš„äºŒè¿›åˆ¶ï¼‰\n")
            f.write("# æ ¼å¼: episode, step, binary_path, score\n")
            for item in episode_binaries:
                f.write(f"{item['episode']},{item['step']},{item['binary']},{item['score']:.4f}\n")
        logger.info(f"âœ“ å›åˆäºŒè¿›åˆ¶æ¸…å•å·²ä¿å­˜: {manifest_path} ({len(episode_binaries)} ä¸ªæ–‡ä»¶)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PPO Trainer for Binary Perturbation')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--binary', required=True, help='åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--function', required=True, help='ç›®æ ‡å‡½æ•°å')
    parser.add_argument('--save-path', required=True, help='å˜å¼‚ç»“æœä¿å­˜è·¯å¾„')
    
    # PPO å‚æ•°
    parser.add_argument('--state-dim', type=int, default=64, help='çŠ¶æ€ç»´åº¦ï¼ˆæ¨è 64ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡ï¼ˆå¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§ï¼‰')
    parser.add_argument('--gamma', type=float, default=0.95, help='æŠ˜æ‰£å› å­ï¼ˆé™ä½ä»¥å‡å°‘æœªæ¥å¥–åŠ±å½±å“ï¼‰')
    parser.add_argument('--epsilon', type=float, default=0.2, help='PPO è£å‰ªå‚æ•°')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--episodes', type=int, default=50, help='è®­ç»ƒå›åˆæ•°ï¼ˆå‡å°‘ä½†æ›´ç¨³å®šï¼‰')
    parser.add_argument('--max-steps', type=int, default=50, help='æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼ˆå‡å°‘ä»¥åŠ å¿«è¿­ä»£ï¼‰')
    parser.add_argument('--save-interval', type=int, default=10, help='ä¿å­˜é—´éš”')
    parser.add_argument('--model-dir', default='./rl_models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', default=None, help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--use-gpu', action='store_true', help='ä½¿ç”¨ GPU')
    
    args = parser.parse_args()
    os.remove(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'log/uroboro.log'))
    train_ppo(args)

