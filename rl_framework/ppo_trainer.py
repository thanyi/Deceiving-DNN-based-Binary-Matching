#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PPO Trainer for Binary Code Perturbation
PPO è®­ç»ƒå™¨ï¼ˆç›´æ¥è°ƒç”¨ç¯å¢ƒï¼‰
"""

import os
import numpy as np
import torch
from ppo_agent import PPOAgent, RewardShaper
import argparse
from loguru import logger
import sys

# å¯¼å…¥ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env_wrapper import BinaryPerturbationEnv


def train_ppo(args):
    """
    PPO è®­ç»ƒä¸»å‡½æ•°
    """
    logger.info("PPO è®­ç»ƒå¯åŠ¨")
    logger.info(f"åŸå§‹äºŒè¿›åˆ¶: {args.binary}")
    logger.info(f"ç›®æ ‡å‡½æ•°: {args.function}")
    logger.info(f"ä¿å­˜è·¯å¾„: {args.save_path}")
    logger.info(f"æœ€å¤§å›åˆæ•°: {args.episodes}")
    logger.info(f"æœ€å¤§æ­¥æ•°/å›åˆ: {args.max_steps}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_path, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    # åˆå§‹åŒ–ç¯å¢ƒï¼ˆç›´æ¥åˆ›å»ºï¼Œæ— éœ€è¿›ç¨‹é€šä¿¡ï¼‰
    logger.info("åˆå§‹åŒ–å˜å¼‚ç¯å¢ƒ...")
    env = BinaryPerturbationEnv(
        original_binary=args.binary,
        function_name=args.function,
        save_path=args.save_path
    )
    logger.info("ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ âœ“")
    
    # åˆå§‹åŒ– PPO Agent
    agent = PPOAgent(
        state_dim=args.state_dim,
        n_actions=8,
        lr=args.lr,
        gamma=args.gamma,
        epsilon=args.epsilon,
        device='cuda' if torch.cuda.is_available() and args.use_gpu else 'cpu'
    )
    
    # å¦‚æœå­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åŠ è½½
    if args.resume and os.path.exists(args.resume):
        agent.load(args.resume)
    
    # å¥–åŠ±å¡‘å½¢å™¨
    reward_shaper = RewardShaper(target_score=0.40)
    
    # è®­ç»ƒæ—¥å¿—
    log_file = os.path.join(args.model_dir, 'training_log.txt')
    best_score = float('inf')
    success_count = 0
    
    try:
        for episode in range(args.episodes):
            logger.info("=" * 80)
            logger.info(f"å›åˆ {episode + 1}/{args.episodes}")
            logger.info("=" * 80)
            
            # é‡ç½®ç¯å¢ƒ
            state = env.reset()
            reward_shaper.reset()
            
            episode_reward = 0
            episode_loss = 0
            
            for step in range(args.max_steps):
                # é€‰æ‹©åŠ¨ä½œ
                action_idx, actual_action, log_prob, value = agent.select_action(state, explore=True)
                
                logger.debug(f"æ­¥éª¤ {step + 1}: åŠ¨ä½œ={actual_action} (ç´¢å¼•={action_idx})")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(actual_action)
                
                # å¥–åŠ±å¡‘å½¢
                if 'score' in info:
                    shaped_reward = reward_shaper.compute_reward(
                        info['score'], 
                        info.get('grad', 0), 
                        done, 
                        step
                    )
                else:
                    shaped_reward = reward  # å¦‚æœæ²¡æœ‰ scoreï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¥–åŠ±
                
                if 'score' in info:
                    logger.info(f"  å¥–åŠ±: {reward:.4f} â†’ {shaped_reward:.4f} | åˆ†æ•°: {info['score']:.4f} | æ¢¯åº¦: {info.get('grad', 0):.4f}")
                else:
                    logger.info(f"  å¥–åŠ±: {reward:.4f}")
                
                # å­˜å‚¨ç»éªŒ
                agent.store_transition(state, action_idx, shaped_reward, log_prob, value)
                
                episode_reward += shaped_reward
                state = next_state
                
                # æ£€æŸ¥æˆåŠŸ
                if done:
                    if 'score' in info and info['score'] < 0.40:
                        success_count += 1
                        logger.success(f"ğŸ‰ æˆåŠŸç»•è¿‡æ£€æµ‹! åˆ†æ•°: {info['score']:.4f}")
                        
                        # ä¿å­˜æˆåŠŸæ ·æœ¬ä¿¡æ¯
                        success_log = os.path.join(args.save_path, 'success.log')
                        with open(success_log, 'a') as f:
                            f.write(f"Episode {episode}, Step {step}, Score: {info['score']:.4f}\n")
                            f.write(f"Binary: {info.get('binary', 'unknown')}\n\n")
                    
                    logger.info(f"å›åˆç»“æŸ (æ­¥æ•°: {step + 1})")
                    break
            
            # PPO æ›´æ–°
            loss = agent.update()
            episode_loss = loss
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            avg_reward = episode_reward / (step + 1)
            
            logger.info(f"å›åˆæ€»ç»“: æ€»å¥–åŠ±={episode_reward:.4f} | å¹³å‡å¥–åŠ±={avg_reward:.4f} | ç­–ç•¥æŸå¤±={loss:.4f} | æˆåŠŸæ¬¡æ•°={success_count}")
            
            # ä¿å­˜åˆ°æ—¥å¿—
            with open(log_file, 'a') as f:
                f.write(f"{episode},{step+1},{episode_reward:.4f},{avg_reward:.4f},{loss:.4f}\n")
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if (episode + 1) % args.save_interval == 0:
                model_path = os.path.join(args.model_dir, f'ppo_model_ep{episode+1}.pt')
                agent.save(model_path)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if 'score' in info and info['score'] < best_score:
                best_score = info['score']
                best_model_path = os.path.join(args.model_dir, 'ppo_model_best.pt')
                agent.save(best_model_path)
                logger.success(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (åˆ†æ•°: {best_score:.4f})")
    
    except KeyboardInterrupt:
        logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(args.model_dir, 'ppo_model_final.pt')
        agent.save(final_model_path)
        
        logger.info("=" * 80)
        logger.success("è®­ç»ƒå®Œæˆ")
        logger.info(f"æˆåŠŸç»•è¿‡æ¬¡æ•°: {success_count} | æœ€ä½³åˆ†æ•°: {best_score:.4f}")
        logger.info("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PPO Trainer for Binary Perturbation')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--binary', required=True, help='åŸå§‹äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--function', required=True, help='ç›®æ ‡å‡½æ•°å')
    parser.add_argument('--save-path', required=True, help='å˜å¼‚ç»“æœä¿å­˜è·¯å¾„')
    
    # PPO å‚æ•°
    parser.add_argument('--state-dim', type=int, default=128, help='çŠ¶æ€ç»´åº¦')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--epsilon', type=float, default=0.2, help='PPO è£å‰ªå‚æ•°')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--episodes', type=int, default=100, help='è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--max-steps', type=int, default=50, help='æ¯å›åˆæœ€å¤§æ­¥æ•°')
    parser.add_argument('--save-interval', type=int, default=10, help='ä¿å­˜é—´éš”')
    parser.add_argument('--model-dir', default='./rl_models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', default=None, help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--use-gpu', action='store_true', help='ä½¿ç”¨ GPU')
    
    args = parser.parse_args()
    
    train_ppo(args)

